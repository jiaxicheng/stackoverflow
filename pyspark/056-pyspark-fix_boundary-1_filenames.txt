https://stackoverflow.com/questions/62063406/how-to-read-csv-which-is-splitted-into-multiple-files-in-pyspark-with-scattered
    
Process data boundaries cross partitions. In this example, the csv file are broken, the last line of a file is broken into
the first line of the next file. to handle this issue, filename must be sortable, and we rely on filenames to identify the connection between lines in various files.
---

    from pyspark.sql.functions import ( when, struct, concat_ws, col, lead, row_number 
        substring_index, input_file_name, monotonically_increasing_id, min as fmin, max as fmax
    )
         
    df = spark.read.text("/home/xicheng/test/csv-3")

(1) find filename and set up an id to sort the Rows
    
    df1 = df.withColumn('fname', substring_index(input_file_name(),'/',-1)) \
        .withColumn('id', monotonically_increasing_id())
    
    +-------------+------+---+
    |        value| fname| id|
    +-------------+------+---+
    |     9878,RSP|a2.txt|  0|
    |5,9878987,CTP|a2.txt|  1|
    |6,9887987,CNR|a2.txt|  2|
    | 7,8789799,PO|a2.txt|  3|
    |1,8767687,ERS|a1.txt|  4|
    |2,8798089,CTP|a1.txt|  5|
    |3,9879879,POI|a1.txt|  6|
    |        4,987|a1.txt|  7|
    |            L|a3.txt|  8|
    |8,9879879,LOR|a3.txt|  9|
    |9,8979879,IIO|a3.txt| 10|
    |10,876998,IYK|a3.txt| 11|
    +-------------+------+---+

(2) create df2 containing the first and last rows of each fname and then normalize the result

    df2 = df1.groupby('fname').agg(
        fmin(struct('id','value')).alias('rmin'),
        fmax(struct('id','value')).alias('rmax')
    ).selectExpr("fname", "stack(2,0,rmin,1,rmax) as (k, val)")
    
    df2.orderBy('fname','k').show()                                                                                         
    +------+---+-------------------+                                                
    | fname|  k|                val|
    +------+---+-------------------+
    |a1.txt|  0| [4, 1,8767687,ERS]|
    |a1.txt|  1|         [7, 4,987]|
    |a2.txt|  0|      [0, 9878,RSP]|
    |a2.txt|  1|  [3, 7,8789799,PO]|
    |a3.txt|  0|             [8, L]|
    |a3.txt|  1|[11, 10,876998,IYK]|
    +------+---+-------------------+
    
(3) create a window to sort the data by fname and k, concatenat the next row when k == 1

    w1 = Window.orderBy('fname','k')

    df3 = df2.withColumn('value', struct(
        when(df2.k == 1, concat_ws('', col('val').value, lead('val').over(w1).value)).otherwise(col('val').value).alias('value'),
        row_number().over(w1).alias('pid'),
        col('val').id.alias('id')
    )).filter('pid = 1 OR k > 0')
    +------+---+-------------------+----------------------+
    |fname |k  |val                |value                 |
    +------+---+-------------------+----------------------+
    |a1.txt|0  |[4, 1,8767687,ERS] |[1,8767687,ERS, 1, 4] |
    |a1.txt|1  |[7, 4,987]         |[4,9879878,RSP, 2, 7] |
    |a2.txt|1  |[3, 7,8789799,PO]  |[7,8789799,POL, 4, 3] |
    |a3.txt|1  |[11, 10,876998,IYK]|[10,876998,IYK, 6, 11]|
    +------+---+-------------------+----------------------+

(4) remove boundary rows in df1 (all ids in df2)     and then union the result with df3 data
    
    df4 = df1.join(df2, df1.id == df2.val.id, 'left_anti').union(df3.selectExpr('value.value', 'fname', 'value.id'))
    +-------------+------+---+                                                      
    |        value| fname| id|
    +-------------+------+---+
    |5,9878987,CTP|a2.txt|  1|
    |6,9887987,CNR|a2.txt|  2|
    |2,8798089,CTP|a1.txt|  5|
    |3,9879879,POI|a1.txt|  6|
    |8,9879879,LOR|a3.txt|  9|
    |9,8979879,IIO|a3.txt| 10|
    |1,8767687,ERS|a1.txt|  4|
    |4,9879878,RSP|a1.txt|  7|
    |7,8789799,POL|a2.txt|  3|
    |10,876998,IYK|a3.txt| 11|
    +-------------+------+---+
    
(5) read_csv using the dataframe created from the above

    df_new = spark.read.csv(df4.select('value').rdd.map(lambda x: x.value), schema='id int,c1 string,c2 string')
    +---+-------+---+
    | id|     c1| c2|
    +---+-------+---+
    |  5|9878987|CTP|
    |  6|9887987|CNR|
    |  2|8798089|CTP|
    |  3|9879879|POI|
    |  8|9879879|LOR|
    |  9|8979879|IIO|
    |  1|8767687|ERS|
    |  4|9879878|RSP|
    |  7|8789799|POL|
    | 10| 876998|IYK|
    +---+-------+---+

  Notes: 
   (1) extra condition should be applied when the last line of a file is broken properly thus the concatenation of
       the last line with the first line of the next file actually contains two Rows.
   (2) an good example for spark.read.csv to use RDD as input path

