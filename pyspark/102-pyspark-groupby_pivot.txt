Examples using pyspark.sql.GroupedData().pivot


---
Example-1: pivot with Months as columns
  REF: https://stackoverflow.com/questions/59634932/issue-with-pyspark-dataframe-with-redundant-values
  Method: Use groupby + pivot
  ---
  (1) pivot to use a list (customize order of columns)
  (2) convert month from number to Str: `date_format(to_date(concat(2020,MONTH), 'yyyyMM'),'MMM')`

    from pyspark.sql.functions import expr, first
    from datetime import datetime

    df = spark.createDataFrame([
        ('Paax', 2019, 12, 'ERG2', '435911'), ('Paax', 2019, 11, 'ELE', '435911'),
        ('Paax', 2019, 11, 'PHA', '435911'), ('Paax', 2019, 12, 'ELE', '435911'),
        ('Paax', 2019, 12, 'EBM', '512518'), ('Paax', 2019, 12, 'PHA', '435911')
    ], ['CLIENT_NAME', 'YEAR', 'MONTH', 'ENGINE', 'TOTAL_UNIQUE_MEMBER']) 

    months = [ datetime(2019,m+1,1).strftime("%b").upper() for m in range(12) ]
    #['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']

    df_new = (df.withColumn('month', expr("upper(date_format(to_date(concat(2020,MONTH), 'yyyyMM'),'MMM'))"))
        .withColumnRenamed('ENGINE', 'ANALYTIC')
        .groupby('ANALYTIC')
        .pivot('month', months)
        .agg(first('TOTAL_UNIQUE_MEMBER'))
        .fillna(''))

    df_new.show()
    +--------+---+---+---+---+---+---+---+---+---+---+------+------+                
    |ANALYTIC|JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|   NOV|   DEC|
    +--------+---+---+---+---+---+---+---+---+---+---+------+------+
    |     PHA|   |   |   |   |   |   |   |   |   |   |435911|435911|
    |    ERG2|   |   |   |   |   |   |   |   |   |   |      |435911|
    |     ELE|   |   |   |   |   |   |   |   |   |   |435911|435911|
    |     EBM|   |   |   |   |   |   |   |   |   |   |      |512518|
    +--------+---+---+---+---+---+---+---+---+---+---+------+------+



Example-2: sum of array elements depending on the value condition
  REF: https://stackoverflow.com/questions/59931770
  MEthod: Use pivot, specify list of values to pivot, this is good for performance:

    from pyspark.sql.functions import sum as fsum, expr

    df = spark.createDataFrame([
        (1,[0.2, 2.1, 3., 4., 3., 0.5]),
        (2,[7., 0.3, 0.3, 8., 2.,]), 
        (3,None), 
        (4,[])
      ],['id','column'])
    df.show()
    +---+--------------------+
    | id|              column|
    +---+--------------------+
    |  1|[0.2, 2.1, 3.0, 4...|
    |  2|[7.0, 0.3, 0.3, 8...|
    |  3|                null|
    |  4|                  []|
    +---+--------------------+

    df.selectExpr('id', 'explode_outer(column) as item') \
      .withColumn('g', expr('if(item < 2, "column<2", if(item > 2, "column>2", "column=2"))')) \
      .groupby('id') \
      .pivot('g', ["column<2", "column>2", "column=2"]) \
      .agg(fsum('item')) \
      .show()
    +---+--------+--------+--------+                                                
    | id|column<2|column>2|column=2|
    +---+--------+--------+--------+
    |  1|     0.7|    12.1|    null|
    |  3|    null|    null|    null|
    |  2|     0.6|    15.0|     2.0|
    |  4|    null|    null|    null|
    +---+--------+--------+--------+


Example-3: merge two columns before doing pivot:
  REF: https://stackoverflow.com/q/64353811/9510729
  Method: the pivot list are from concatenation of two columns, see below `p1` column

    from pyspark.sql.functions import expr, first

    df = spark.createDataFrame([
        (2, 'two', 0, 'Hybrid', 58), (2, 'two', 1, 'Hybrid', 2), (5, 'five', 1, 'Excl', 13), 
        (5, 'five', 0, 'Excl', 70), (5, 'five', 0, 'Agen', 811), (5, 'five', 1, 'Agen', 279),
        (5, 'five', 1, 'Hybrid', 600), (5, 'five', 0, 'Hybrid', 2819)     
    ], ['id', 'name', 'policy', 'payment_name', 'count'])

    df.withColumn('p1', expr("concat_ws('_', IF(policy=0,'no',NULL),'policy',lower(payment_name))")) \
        .groupby('id','name') \
        .pivot('p1') \
        .agg(first('count')) \
        .fillna(0) \
        .show()
    +---+----+--------------+--------------+----------------+-----------+-----------+-------------+
    | id|name|no_policy_agen|no_policy_excl|no_policy_hybrid|policy_agen|policy_excl|policy_hybrid|
    +---+----+--------------+--------------+----------------+-----------+-----------+-------------+
    |  2| two|             0|             0|              58|          0|          0|            2|
    |  5|five|           811|            70|            2819|        279|         13|          600|
    +---+----+--------------+--------------+----------------+-----------+-----------+-------------+



Example-4: similar to example-3, stack/merge three columns and set up the pivot column
  REF: https://stackoverflow.com/q/64499593/9510729
  Method: use stack to normalize tag_name and (tag_value + Tag_stat), concat these two columns
          and then do the pivot.
  Notice: column inside the pivot argument can not be an expression, must be a single column name

    from pyspark.sql import functions as F

    df = spark.createDataFrame([
        ('a1', 'Work', 'passHolder', 'Jack Ryan', 'verified', 1.5),
        ('a1', 'Work', 'passNum', '1234', 'unverified', 1.5) 
    ], ['Id', 'Type', 'Tag_name', 'Tag_value', 'Tag_stat', 'version']) 

    df.selectExpr("Id", "Type", "Tag_name", "stack(2,'Tag', Tag_value, 'stat', Tag_stat) as (value, stat)", "version") \
      .withColumn('p', F.concat_ws('_','Tag_name','value')) \
      .groupby('Id', 'Type','version') \
      .pivot('p') \
      .agg(F.first('stat')) \
      .show()
    +---+----+-------+--------------+---------------+-----------+------------+      
    | Id|Type|version|passHolder_Tag|passHolder_stat|passNum_Tag|passNum_stat|
    +---+----+-------+--------------+---------------+-----------+------------+
    | a1|Work|    1.5|     Jack Ryan|       verified|       1234|  unverified|
    +---+----+-------+--------------+---------------+-----------+------------+



Example-5: unpivot + pivot + full-join for dot-product:
  REF: https://stackoverflow.com/questions/65145134
  Target: given a pivot-table of user-page_clicks matrix, convert it to page-page matrix:

    from pyspark.sql import functions as F

    df_pivot = spark.createDataFrame([
        ('user1', 1, 1, None, None), 
        ('user2', 1, 1, None, None), 
        ('user3', None, 1, 1, None), 
        ('user4', None, None, None, 1)
    ], ['UserId', 'productA', 'itemB', 'articleC', 'objectD'])

    cols = df_pivot.columns[1:]

    # unpivot/normalize the df_pivot to userId vs target
    df1 = df_pivot.select(
        'userId', 
        F.explode(F.split(F.concat_ws('|', *[F.when(F.col(c).isNotNull(), F.lit(c)) for c in cols]),'\|')).alias('target')
    )
    #df1.show()
    #+------+--------+
    #|userId|  target|
    #+------+--------+
    #| user1|productA|
    #| user1|   itemB|
    #| user2|productA|
    #| user2|   itemB|
    #| user3|   itemB|
    #| user3|articleC|
    #| user4| objectD|
    #+------+--------+

    # self full-outer join
    df2 = df1.join(df1.withColumnRenamed('target','target_1'),'userId','full')

    # pivot
    df_new = df2.groupby('target') \
        .pivot('target_1', cols) \
        .agg(F.countDistinct('userId')) \
        .fillna(0, subset=cols)
        
    #df_new.show()
    #+--------+--------+-----+--------+-------+
    #|  target|productA|itemB|articleC|objectD|
    #+--------+--------+-----+--------+-------+
    #|productA|       2|    2|       0|      0|
    #|   itemB|       2|    3|       1|      0|
    #|articleC|       0|    1|       1|      0|
    #| objectD|       0|    0|       0|      1|
    #+--------+--------+-----+--------+-------+


