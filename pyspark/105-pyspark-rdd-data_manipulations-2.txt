Continue..

RDD data manipulations:


Example-6: dictionary as RDD elements, fix not hashable ERROR:
  REF: https://stackoverflow.com/questions/61967644/how-to-get-distinct-dicts-with-nested-list-of-rdd-in-pyspark

  If the RDD elements are dictionaries containing a single key. Below is one way to try:

  Set up RDD as below:

    rdd = sc.parallelize([
        {"link0":["link1","link2"]}, {"link1":["link2","link3"]},  
        {"link0":["link1","link2"]}, {"link3":["link4","link5"]}]) 

  Task-1: find unique RDD elements:

  Use flatMap to convert the dict to a tuple with the valueipart from list to tuple so that the RDD elements are hashable.
  take distinct() and then map the RDD elements back to their original data structure.

    rdd.flatMap(lambda x: [ (k,tuple(v)) for k,v in x.items() ]) \
       .distinct() \
       .map(lambda x: {x[0]:list(x[1])}) \
       .collect()
    #[{'link0': ['link1', 'link2']},
    # {'link1': ['link2', 'link3']},
    # {'link3': ['link4', 'link5']}]


  Task-2: find links in values not in keys:

  retrieve all unique keys into rdd1 and unique values to rdd2 and then do `rdd2.subtract(rdd1)`

    rdd1 = rdd.flatMap(lambda x: x.keys()).distinct()
    # ['link0', 'link1', 'link3']

    rdd2 = rdd.flatMap(lambda x: [ v for vs in x.values() for v in vs ]).distinct()
    # ['link1', 'link2', 'link3', 'link4', 'link5']

    rdd2.subtract(rdd1).collect()
    # ['link2', 'link5', 'link4']


  Notes: for Task-1 to handle more complex dicts, see more than one key but values can contains list (no set, dict etc)
   (1) dict keys must be sorted
   (2) values will be converted to tuple if `isinstance(v,list)`

    rdd = sc.parallelize([
        {"link0":["link1","link2"],"topic":1}, 
        {"link1":["link2","link3"],"topic":2},  
        {"link0":["link1","link2"],"topic":1},
        {"link3":["link4","link5"],"topic":3}
    ])

    rdd.map(lambda x: tuple((k, tuple(v) if isinstance(v,list) else v) for k,v in sorted(x.items()))) \
       .distinct() \
       .map(lambda x: dict( (e[0], list(e[1]) if isinstance(e[1], tuple) else e[1]) for e in x)) \
       .collect()



Example-7: use combinations and zip:
  REF:https://stackoverflow.com/questions/63494362/creating-combination-and-sum-of-value-lists-with-existing-key-pyspark
  Target: find combination of two zipped lists:

    from itertools import combinations 
    rdd = sc.parallelize([('k1', ['v11', 'v12', 'v13'], [1,2,3]),('k2', ['v21', 'v22', 'v23'], [10,20,30])]) 

    def my_combinations(row):
      return [ (row[0], tuple(e[0] for e in c), sum(e[1] for e in c)) for c in combinations(zip(*row[1:]),2) ]

    rdd.flatMap(my_combinations).collect()
    #[('k1', ('v11', 'v12'), 3),
    # ('k1', ('v11', 'v13'), 4),
    # ('k1', ('v12', 'v13'), 5),
    # ('k2', ('v21', 'v22'), 30),
    # ('k2', ('v21', 'v23'), 40),
    # ('k2', ('v22', 'v23'), 50)]

  Some notes: 
   (1) find the first items of a list of lists:

    Method-1: using list-comprehension:   tuple(e[0] for e in c) 
    Method-2: using zip               :   tuple([*zip(*c)][0])
    Method-3: using map and itemgetter:   from operator import itemgetter
                                          tuple(map(itemgetter(0), c))



