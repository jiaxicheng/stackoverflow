# Use sequence(start_T, end_T,[,step_T]): array<T>
available Spark 2.4.0+

Note: For temporal(of or relating to time) sequence, the default step_T is 1 day or -1 day:
example: SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month);


---
Example-1: duplicate-rows-according-to-a-condition-with-explode-and-array
  REF: https://stackoverflow.com/questions/58492974
  For **Spark 2.4.0+**, use **[sequence][1]** + **[transform][2]** + explode to create new Rows with this task:

    from pyspark.sql.functions import expr

    df_new = (df
        .withColumn('s_date', expr("last_day(to_date(StartDate, 'M/d/yyyy'))"))
        .withColumn('e_date', expr("last_day(IFNULL(to_date(EndDate, 'M/d/yyyy'), add_months(current_date(),-1)))"))
        .withColumn('EndOfTheMonth', expr('''
              explode_outer(transform(
                sequence(0, int(months_between(e_date, s_date))), i -> add_months(s_date,i)
              ))
         '''))
        .withColumn('IsDeliveryOpen', expr("IF(e_date > EndOfTheMonth or EndDate is Null, 1, 0)"))
    )

    df_new.show()
    +---------+---------+--------------+---------+------------+----------+----------+-------------+--------------+
    |Reference|StartDate|StartTimestamp|  EndDate|EndTimestamp|    s_date|    e_date|EndOfTheMonth|IsDeliveryOpen|
    +---------+---------+--------------+---------+------------+----------+----------+-------------+--------------+
    |        1|8/15/2019|           SOD|9/18/2019|         EOD|2019-08-31|2019-09-30|   2019-08-31|             1|
    |        1|8/15/2019|           SOD|9/18/2019|         EOD|2019-08-31|2019-09-30|   2019-09-30|             0|
    |        2|8/16/2019|           SOD|8/23/2019|         EOD|2019-08-31|2019-08-31|   2019-08-31|             0|
    |        3|6/17/2019|           SOD| 8/4/2019|         EOD|2019-06-30|2019-08-31|   2019-06-30|             1|
    |        3|6/17/2019|           SOD| 8/4/2019|         EOD|2019-06-30|2019-08-31|   2019-07-31|             1|
    |        3|6/17/2019|           SOD| 8/4/2019|         EOD|2019-06-30|2019-08-31|   2019-08-31|             0|
    |        4| 8/1/2019|           SOD|     null|        null|2019-08-31|2019-09-30|   2019-08-31|             1|
    |        4| 8/1/2019|           SOD|     null|        null|2019-08-31|2019-09-30|   2019-09-30|             1|
    +---------+---------+--------------+---------+------------+----------+----------+-------------+--------------+

    df_new = df_new.drop('s_date', 'e_date')

**How it works:**

  1. convert *StartDate*, *EndDate* to DateType with the value to the last_day of the same month(*s_date*, *e_date*). 
     if *EndDate* is NULL, then set its value to last_day of the previous month from the current_date

  2. calculate *# of months* between the above two dates and then create a sequence(0, #months) and transform it 
     into an array of months(`EndOfTheMonth`) between StartDate and EndDate (inclusively)

  3. use explode_outer to generate Rows for all months in the above array

  4. calculate the IsDeliveryOpen flag accordingly. I removed `StartDate <= EndOfTheMonth` in your code since it's 
     always true based on how *EndOfTheMonth* is calculated.

  **Note:** the above can also be written as one SQL statement:

    df.createOrReplaceTempView('t_df')

    spark.sql('''

        WITH d AS (
            SELECT *
                 , last_day(to_date(StartDate, 'M/d/yyyy')) as s_date
                 , last_day(IFNULL(to_date(EndDate, 'M/d/yyyy'),add_months(current_date(),-1))) as e_date
            FROM t_df
        )
        SELECT d.*
             , m.EndOfTheMonth
             , IF(e_date > m.EndOfTheMonth or d.EndDate is NULL,1,0) AS IsDeliveryOpen
        FROM d
        LATERAL VIEW OUTER explode(
            transform(sequence(0, int(months_between(e_date, s_date))), i -> add_months(s_date,i))
        ) m AS EndOfTheMonth

    ''').show()

  #################################################
  More request to handle the same in weekly basis:
  #################################################
  """
  Use date_trunc('WEEK', date_col) which truncate the date_col to the Monday of the same week.
  Use sequence(start_date, end_date, interval 7 days) to generate the sequence
  """

  df_weekly = (df
    .withColumn('s_date', expr("date(date_trunc('WEEK', to_date(StartDate, 'M/d/yyyy')))"))
    .withColumn('e_date', expr("date(date_trunc('WEEK', IFNULL(to_date(EndDate, 'M/d/yyyy'), date_sub(current_date(),7))))"))
    .withColumn('StartOfTheWeek', expr('explode_outer(sequence(s_date, e_date, interval 7 days))'))
    .withColumn('IsDeliveryOpen', expr("IF(e_date > StartOfTheWeek or EndDate is Null, 1, 0)"))
  )


  # Or in the Spark SQL syntax:
  df.createOrReplaceTempView('t_df')

  spark.sql('''

    WITH d AS (
        SELECT *
             , date(date_trunc('WEEK', to_date(StartDate, 'M/d/yyyy'))) AS s_date
             , date(date_trunc('WEEK', IFNULL(to_date(EndDate, 'M/d/yyyy'), date_sub(current_date(),7)))) AS e_date
        FROM t_df
    )
    SELECT d.*
         , w.StartOfTheWeek
         , IF(e_date > w.StartOfTheWeek or d.EndDate is NULL,1,0) AS IsDeliveryOpen
    FROM d
    LATERAL VIEW OUTER explode(sequence(s_date, e_date, interval 7 days)) w AS StartOfTheWeek

  ''')


  [1]: https://spark.apache.org/docs/2.4.0/api/sql/index.html#sequence
  [2]: https://spark.apache.org/docs/2.4.0/api/sql/index.html#transform


Example-2: 
  REF: https://docs.databricks.com/_static/notebooks/apache-spark-2.4-functions.html
    
    df1 = spark.createDataFrame([
            (1, '2019-01-01', '2019-01-07')
          , (2, '2019-02-02', '2019-02-05')
          , (3, '2019-03-03', '2019-03-15')
        ], ['id', 'start_date', 'end_date']
    )
       # , schema='struct<id:int,start_date:date,end_date:date>'
    
    df_new = df1.withColumn('drange', F.explode(F.expr("sequence(to_date(start_date), to_date(end_date), interval 1 day)")))
    
    df_new.show(50)
    +---+----------+----------+----------+
    | id|start_date|  end_date|    drange|
    +---+----------+----------+----------+
    |  1|2019-01-01|2019-01-07|2019-01-01|
    |  1|2019-01-01|2019-01-07|2019-01-02|
    |  1|2019-01-01|2019-01-07|2019-01-03|
    |  1|2019-01-01|2019-01-07|2019-01-04|
    |  1|2019-01-01|2019-01-07|2019-01-05|
    |  1|2019-01-01|2019-01-07|2019-01-06|
    |  1|2019-01-01|2019-01-07|2019-01-07|
    |  2|2019-02-02|2019-02-05|2019-02-02|
    |  2|2019-02-02|2019-02-05|2019-02-03|
    |  2|2019-02-02|2019-02-05|2019-02-04|
    |  2|2019-02-02|2019-02-05|2019-02-05|
    |  3|2019-03-03|2019-03-15|2019-03-03|
    |  3|2019-03-03|2019-03-15|2019-03-04|
    |  3|2019-03-03|2019-03-15|2019-03-05|
    |  3|2019-03-03|2019-03-15|2019-03-06|
    |  3|2019-03-03|2019-03-15|2019-03-07|
    |  3|2019-03-03|2019-03-15|2019-03-08|
    |  3|2019-03-03|2019-03-15|2019-03-09|
    |  3|2019-03-03|2019-03-15|2019-03-10|
    |  3|2019-03-03|2019-03-15|2019-03-11|
    +---+----------+----------+----------+
    
  **WARNING**: 
  + be cautuin using `Interval 1 month` with the sequence() function, it will not generate dates
    for each month, instead, it's more like `interval 30 day`. To create a list of consecutive months
    the approach might be the following:
    (1) d = explode(sequence(start_date, end_date, interval 28 day))
    (2) d = last_day(d)
    (3) run drop_duplicates() on the dataframe

 

Example-3: how-do-i-use-flatmap-with-multiple-columns-in-dataframe
  REF: https://stackoverflow.com/questions/60068037
  Method: Use sequence to generate an array of timestamps between starttime and endtime interval by 1 hours, 
          and then transform it into a named_struct with interval calculated by each item (x below) of the 
          above sequence

    df.withColumn('dts', expr("""
      transform(
        /* create a sequence between the HOURS of starttime and endtime */
        sequence(date_trunc("HOUR", starttime), date_trunc("HOUR", endtime), interval 1 hours),
        /* iterate through each item `x` of sequence and convert it into a named_struct */
        x -> named_struct(
          /* starttime is the max between x and starttime */
          'starttime', greatest(x, starttime), 
          /* endtime is the min between x + 59min + 59sec and endtime */
          'endtime', least(x + interval 59 minutes 59 seconds, endtime)
        )
      )
    """)).selectExpr('Name', 'city', 'inline_outer(dts)').show()
    +-----+------+-------------------+-------------------+
    |Name |city  |starttime          |endtime            |
    +-----+------+-------------------+-------------------+
    |user1|London|2019-08-02 03:34:45|2019-08-02 03:52:03|
    |user2|Boston|2019-08-13 13:34:10|2019-08-13 13:59:59|
    |user2|Boston|2019-08-13 14:00:00|2019-08-13 14:59:59|
    |user2|Boston|2019-08-13 15:00:00|2019-08-13 15:02:10|
    +-----+------+-------------------+-------------------+

  **Where:**
  (1) `date_trunc("HOUR", starttime)`: truncate the column starttime to HOUR
  (2) `sequence(start, end, interval 1 hours)`: create a sequence of timestamp between `start` and `end`, 
      interval by 1 hours inclusively
  (3) `transform(arr_col, x -> names_struct(..))`: iterate through each array item x in arr_col 
      and transform it into a named_struct



Example-4: populate missing dates using sequence + transform + inline
  REF: https://stackoverflow.com/questions/64466161
  Method: using transform + sequence + inline functions

    from pyspark.sql.functions import expr, lead, date_add, coalesce
    from pyspark.sql import Window

    df = spark.createDataFrame([("2020-10-01",10),("2020-10-03",15),("2020-10-06",16)],["date", "value"])

    # this is only a sample, real data have columns to set up partitionBy()
    w1 = Window.partitionBy().orderBy('date')

    df.withColumn('date', expr("date(date)")) \
        .withColumn('end_date', coalesce(date_add(lead('date').over(w1),-1),'date')) \
        .selectExpr("""
          inline(
            transform(
              sequence(date,end_date), 
              (d,i) -> (d as date, if(i=0,value,NULL) as value)
            )
          )
         """) \
        .show()
    +----------+-----+
    |      date|value|
    +----------+-----+
    |2020-10-01|   10|
    |2020-10-02| null|
    |2020-10-03|   15|
    |2020-10-04| null|
    |2020-10-05| null|
    |2020-10-06|   16|
    +----------+-----+



Example-5: similar to the above Example-4, but fix missing months
  REF: https://stackoverflow.com/q/64486068/9510729
  Task: add missing dates and calculate N-day moving averages
  Notes: add_months() + months_between() to iterate through months correctly. below `end_date` and `date` 
         should be on the same day#, i.e. `01` the first day or the last_day(date)

      sequence(0, int(months_between(end_date, date)))


    from pyspark.sql import functions as F, Window

    df = spark.createDataFrame([
        ('A', 'pharma', '1/3/2019', 50), ('A', 'pharma', '1/4/2019', 60), ('A', 'pharma', '1/5/2019', 70), 
        ('A', 'pharma', '1/8/2019', 80), ('A', 'ENT', '1/8/2019', 50), ('A', 'ENT', '1/9/2019', 65), 
        ('A', 'ENT', '1/11/2019', 40) 
    ], ['product', 'specialty', 'date', 'sales']) 

    df = df.withColumn('date', F.to_date('date', 'd/M/yyyy'))

    w1 = Window.partitionBy('product', 'specialty').orderBy('date')

    df1 = df.withColumn('end_date', F.coalesce(F.add_months(F.lead('date').over(w1),-1),'date')) \
        .selectExpr("product", "specialty", """
            inline_outer(
              transform(
                sequence(0,int(months_between(end_date, date))),
                i -> (add_months(date,i) as date, IF(i=0,sales,0) as sales)
              )
          )""")

    df1.show()
    +-------+---------+----------+-----+                                            
    |product|specialty|      date|sales|
    +-------+---------+----------+-----+
    |      A|      ENT|2019-08-01|   50|
    |      A|      ENT|2019-09-01|   65|
    |      A|      ENT|2019-10-01|    0|
    |      A|      ENT|2019-11-01|   40|
    |      A|   pharma|2019-03-01|   50|
    |      A|   pharma|2019-04-01|   60|
    |      A|   pharma|2019-05-01|   70|
    |      A|   pharma|2019-06-01|    0|
    |      A|   pharma|2019-07-01|    0|
    |      A|   pharma|2019-08-01|   80|
    +-------+---------+----------+-----+

    N = 3
    
    w2 = Window.partitionBy('product', 'specialty').orderBy('date').rowsBetween(-N+1,0)

    df1.select("*", F.round(F.sum('sales').over(w2)/N,2).alias(f'{N}month_avg_sales')).show()
    +-------+---------+----------+-----+----------------+                           
    |product|specialty|      date|sales|3month_avg_sales|
    +-------+---------+----------+-----+----------------+
    |      A|      ENT|2019-08-01|   50|           16.67|
    |      A|      ENT|2019-09-01|   65|           38.33|
    |      A|      ENT|2019-10-01|    0|           38.33|
    |      A|      ENT|2019-11-01|   40|            35.0|
    |      A|   pharma|2019-03-01|   50|           16.67|
    |      A|   pharma|2019-04-01|   60|           36.67|
    |      A|   pharma|2019-05-01|   70|            60.0|
    |      A|   pharma|2019-06-01|    0|           43.33|
    |      A|   pharma|2019-07-01|    0|           23.33|
    |      A|   pharma|2019-08-01|   80|           26.67|
    +-------+---------+----------+-----+----------------+



Example-6: similar to example-5
  Method: 
   (1) find `end_month` using Window aggregate function F.lead and fill the last null value with the current month
   (2) take sequence between 0 and `int(months_between(end_month,month))`, transform i to corresponding month values
       and then explode:
  Code:

    from pyspark.sql import functions as F, Window

    df = spark.createDataFrame([
        (1, 'available', 5, '2020-07'), (1, 'available', 8, '2020-08'), (1, 'limited', 8, '2020-10'), 
        (2, 'limited', 1, '2020-09'), (2, 'limited', 3, '2020-10') 
    ], ['product_id', 'status', 'price', 'month'])

    w1 = Window.partitionBy('product_id').orderBy('month')

    df.withColumn('month', F.to_date('month', 'yyyy-MM')) \
        .withColumn('end_month', F.coalesce(F.add_months(F.lead('month').over(w1),-1), F.trunc(F.current_date(),'MM'))) \
        .selectExpr(
            'product_id',
            'status',
            'price',
            '''
              explode(
                transform(
                  sequence(0, int(months_between(end_month,month))), 
                  i -> date_format(add_months(month,i),'yyyy-MM')
                )
              )
            '''
        ).show()
    +----------+---------+-----+-------+                                            
    |product_id|   status|price|    col|
    +----------+---------+-----+-------+
    |         1|available|    5|2020-07|
    |         1|available|    8|2020-08|
    |         1|available|    8|2020-09|
    |         1|  limited|    8|2020-10|
    |         1|  limited|    8|2020-11|
    |         1|  limited|    8|2020-12|
    |         2|  limited|    1|2020-09|
    |         2|  limited|    3|2020-10|
    |         2|  limited|    3|2020-11|
    |         2|  limited|    3|2020-12|
    +----------+---------+-----+-------+



Example-7: use sequence to take N-gram of an ArrayType column:
  REF: https://stackoverflow.com/questions/65340994/exploding-an-array-into-2-columns

    df = spark.createDataFrame([
        (1,"TTT A B X Y Z CUSTOMER"),
        (2,"YYY E Y F G I P B X Q CUSTOMER"),
    ],["ID","Route"])

    df_new = df.withColumn('dta', F.split('Route',' ')) \
        .selectExpr("ID", "inline(transform(sequence(0,size(dta)-2), i -> (dta[i] as START, dta[i+1] as END)))")

    df_new.show()
    +---+-----+--------+
    | ID|START|     END|
    +---+-----+--------+
    |  1|  TTT|       A|
    |  1|    A|       B|
    |  1|    B|       X|
    |  1|    X|       Y|
    |  1|    Y|       Z|
    |  1|    Z|CUSTOMER|
    |  2|  YYY|       E|
    |  2|    E|       Y|
    |  2|    Y|       F|
    |  2|    F|       G|
    |  2|    G|       I|
    |  2|    I|       P|
    |  2|    P|       B|
    |  2|    B|       X|
    |  2|    X|       Q|
    |  2|    Q|CUSTOMER|
    +---+-----+--------+



Example-8: Aggreate rows into time-buckets
  REF: https://stackoverflow.com/q/65851104/9510729
  Method: see comments in code

    df = spark.createDataFrame([
        ('2019-05-10 14:30:00', '2019-05-10 16:02:00', 'sensor_1'), 
        ('2019-05-11 17:30:00', '2019-05-11 17:35:00', 'sensor_2'), 
        ('2019-05-11 17:40:00', '2019-05-11 17:50:00', 'sensor_2') 
    ], ['from_timestamp', 'to_timestamp', 'sensor_uuid'])

    # covnert two columns into TimestampType
    df = df.withColumn('from_timestamp', F.to_timestamp('from_timestamp')) \
        .withColumn('to_timestamp', F.to_timestamp('to_timestamp'))

    # truncate from_timestamp and to_timestamp to HOUR and then use them to create 
    # a sequence with step=interval 1 hours
    # transform the array elements into a struct<start:timestamp,end:timestamp,bucket_end_time:timestamp>
    # and explode the array of structs
    df1 = df.selectExpr(
        "sensor_uuid", 
        """
          inline(
            transform(
              sequence(date_trunc('hour',from_timestamp), date_trunc('hour',to_timestamp), interval 1 hours), 
              x -> struct(
                greatest(x, from_timestamp) as start, 
                least(x+interval 3599 seconds, to_timestamp) as end, 
                x + interval 1 hours as bucket_end_time)
             )
           )""")
    
    df1.show(truncate=False)
    +-----------+-------------------+-------------------+-------------------+
    |sensor_uuid|start              |end                |bucket_end_time    |
    +-----------+-------------------+-------------------+-------------------+
    |sensor_1   |2019-05-10 14:30:00|2019-05-10 14:59:59|2019-05-10 15:00:00|
    |sensor_1   |2019-05-10 15:00:00|2019-05-10 15:59:59|2019-05-10 16:00:00|
    |sensor_1   |2019-05-10 16:00:00|2019-05-10 16:02:00|2019-05-10 17:00:00|
    |sensor_2   |2019-05-11 17:30:00|2019-05-11 17:35:00|2019-05-11 18:00:00|
    |sensor_2   |2019-05-11 17:40:00|2019-05-11 17:50:00|2019-05-11 18:00:00|
    +-----------+-------------------+-------------------+-------------------+
    
    # groupby sensor_uuid and bucket_end_time and then calculate bucket_use_time
    df1.groupby('sensor_uuid', 'bucket_end_time') \
        .agg(F.expr("sum(unix_timestamp(end)-unix_timestamp(start)+1) as bucket_use_time")) \
        .show()
    +-----------+-------------------+---------------+                               
    |sensor_uuid|    bucket_end_time|bucket_use_time|
    +-----------+-------------------+---------------+
    |   sensor_2|2019-05-11 18:00:00|            902|
    |   sensor_1|2019-05-10 15:00:00|           1800|
    |   sensor_1|2019-05-10 17:00:00|            121|
    |   sensor_1|2019-05-10 16:00:00|           3600|
    +-----------+-------------------+---------------+
    
    

