https://stackoverflow.com/questions/57499828/how-to-match-extract-multi-line-pattern-from-file-in-pysark

Use newAPIHadoopFile
http://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.SparkContext.newAPIHadoopFile

* Read the inpur data in paragraph-mode so that all related lines are read int he same RDD element.

(1) Data Set up:

Sample data: '''
<Q31> <prop/P1082> <Pointer_Q31-87RF> .
<Pointer_Q31-87RF> <rank> <BestRank> .
<Pointer_Q31-87RF> <prop/Pointer_P1082> "+24954"^^<2001/XMLSchema#decimal> .
<Pointer_Q31-87RF> <prop/Pointer_value/P1082> <value/cebcf9> .
<value/cebcf9> <syntax-ns#type> <QuantityValue> .
<value/cebcf9> <quantityAmount> 24954
<value/cebcf9> <quantityUnit> <Meter> .
<Q25> <prop/P1082> <Pointer_Q25-8E6C> .
<Pointer_Q25-8E6C> <rank> <NormalRank> .
<Pointer_Q25-8E6C> <prop/Pointer_P1082> "+24954"
<Pointer_Q25-8E6C> <prop/Pointer_value/P1082> <value/cebcf9> .
<value/cebcf9> <syntax-ns#type> <QuantityValue> .
<value/cebcf9> <quantityAmount> "582" .
<value/cebcf9> <quantityUnit> <Kilometer> .
'''

(2) Python function to parse data.

    from pyspark.sql import Row
    import re

    ctrl_args = { 
        'columns': ['Item_Id', 'quantityAmount', 'quantityUnit', 'rank'],
        'patterns': {
            'quantityAmount': re.compile(r'^quantityAmount>\D*(\d+)'),
            'Item_Id': re.compile(r'^(?:<Q)?(\d+)')
        }
    }

    # below function modified to delimiter with `\n<Q`
    def parse_rdd_element(x, kargs):
        try: 
            row = {}
            for e in x.split('\n'):
                y = e.split('> <')
                if len(y) < 2: 
                    continue
                if y[1] in ['rank', 'quantityUnit']:
                    row[y[1]] = y[2].split(">")[0]
                else:
                    m = re.match(kargs['patterns']['quantityAmount'], y[1])
                    if m: 
                        row['quantityAmount'] = m.group(1)
                        continue
                    m = re.match(kargs['patterns']['Item_Id'], y[0])
                    if m:
                        row['Item_Id'] = 'Q' + m.group(1)
            # if row is not EMPTY, set None to missing field
            return Row(**dict([ (k, row[k]) if k in row else (k, None) for k in kargs['columns']])) if row else None
        except:
            return None

    
(3) Read the data with newAPIHadoopFile()  
  
    rdd = spark.sparkContext.newAPIHadoopFile(
        'file:///home/hdfs/test/pyspark/delimiter-2.txt',
        'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',
        'org.apache.hadoop.io.LongWritable',
        'org.apache.hadoop.io.Text',
        conf={'textinputformat.record.delimiter': '\n<Q'}
    )
    
(4) use map() function to process the RDD element x[1] into Row object 
    and use filter() to skip element w/o a match.

    rdd.map(lambda x: parse_rdd_element(x[1], ctrl_args)).collect()
    [Row(Item_Id=u'Q31', quantityAmount=u'24954', quantityUnit=u'Meter', rank=u'BestRank'),
     Row(Item_Id=u'Q25', quantityAmount=u'582', quantityUnit=u'Kilometer', rank=u'NormalRank')]

(5) convert the RDD into dataframe: 

    df = rdd.map(lambda x: parse_rdd_element(x[1], ctrl_args)).filter(bool).toDF()
    df.show()
    +-------+--------------+------------+----------+
    |Item_Id|quantityAmount|quantityUnit|      rank|
    +-------+--------------+------------+----------+
    |    Q31|         24954|       Meter|  BestRank|
    |    Q25|           582|   Kilometer|NormalRank|
    +-------+--------------+------------+----------+
    

Some notes:
+ textinputformat.record.delimiter does not support regex, for more complex delimiter, will have to
  write your own Hadoop input format class. 

+ In case there is no '\n\n' to separate data chunks in paragraph-mode, we can use '\n<Q' as delimiter
  in the above example if the block of texts are always started with '<Q' at the beginning of the line.
  
  + In case it allows other tags like '\n<P', then you will have to setup your own Hadoop TextInputFormat class.
  + If there are just optional whitespaces in front of '<Q', then it can be resolved by adding a flatMap()
    function, for example:

      df = spark.sparkContext.newAPIHadoopFile(
        'file:///home/hdfs/test/pyspark/delimiter-2.txt',
        'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',
        'org.apache.hadoop.io.LongWritable',
        'org.apache.hadoop.io.Text',
        conf={'textinputformat.record.delimiter': '\n<Q'}
      ).flatMap(lambda x: [ parse_rdd_element(e, ctrl_args) for e in re.split(r'\n\s+<Q', x[1]) if e ]) \
       .filter(bool) \
       .toDF()

