SparkSQL String Manipulation functions:


---
Example-1: mask-replace-inner-part-of-string-column-in-pyspark
  REF:https://stackoverflow.com/questions/58613681/
  Method: retrieving the position of '@' in the string, and then do some math with the Spark SQL functions CONCAT, LEFT, REPEAT and SUBSTR:

    from pyspark.sql.functions import instr, expr

    df = spark.createDataFrame(
            [(e,) for e in ['abc123@gmail.com', '123abc123@yahoo.com', 'abd@gmail.com']]
          , ['email_col']
    ) 

    # set N=2 as a parameter in the SQL expression
    N = 2

    df.withColumn('loc_at', instr('email_col', '@')) \
      .withColumn('new_col', expr("""
            CONCAT(LEFT(email_col,{0}), REPEAT('*', loc_at-2*{0}-1), SUBSTR(email_col, loc_at-{0}))
       """.format(N))).show(truncate=False)
    +-------------------+------+-------------------+
    |email_col          |loc_at|new_col            |
    +-------------------+------+-------------------+
    |abc123@gmail.com   |7     |ab**23@gmail.com   |
    |123abc123@yahoo.com|10    |12*****23@yahoo.com|
    |abd@gmail.com      |4     |abbd@gmail.com     |
    +-------------------+------+-------------------+

  **Note:** to handle email with the length of username less than 5 when N==2, just add
       an IF() statement to the above SQL expression: 

    IF(loc_at < 5
       , CONCAT(LEFT(email_col,1), REPEAT('*', loc_at-3), SUBSTR(email_col, loc_at-1))
       , CONCAT(LEFT(email_col,2), REPEAT('*', loc_at-5), SUBSTR(email_col, loc_at-2))
    )

    # result:
    +-------------------+------+-------------------+
    |email_col          |loc_at|new_col            |
    +-------------------+------+-------------------+
    |abc123@gmail.com   |7     |ab**23@gmail.com   |
    |123abc123@yahoo.com|10    |12*****23@yahoo.com|
    |abd@gmail.com      |4     |a*d@gmail.com      |
    +-------------------+------+-------------------+


  Note: to wrap up the logic into a function:

    from pyspark.sql.functions import expr

    N = 2
    mask_email = lambda x, N: expr("""
    
      IF(INSTR({0}, '@') < {1}*2+2
        , CONCAT(LEFT({0},1), REPEAT('*', INSTR({0}, '@')-2), SUBSTR({0}, INSTR({0}, '@')))
        , CONCAT(LEFT({0},{1}), REPEAT('*', INSTR({0}, '@')-2*{1}-1), SUBSTR({0}, INSTR({0}, '@')-{1})) 
      ) as `{0}_masked`

    """.format(x,N))

    df.select('*', mask_email('email_col', N)).show()
    +-------------------+-------------------+
    |          email_col|   email_col_masked|
    +-------------------+-------------------+
    |   abc123@gmail.com|   ab**23@gmail.com|
    |123abc123@yahoo.com|12*****23@yahoo.com|
    |      abd@gmail.com|      a**@gmail.com|
    +-------------------+-------------------+



Example-2: map values read from one column
  REF: https://stackoverflow.com/questions/60177244/pyspark-udf-hangs-how-to-avoid-udf
  Task: to map values read from one column: for example the original column A = '0/2' and
        map B = 'AGF', we want to convert A into 'A/F'

  The dataframe is as follows:

    df = spark.createDataFrame([('0/1/2/3','AG'),('0/1/3/2','RTFS')],['A','B'])
    
  Method-1: use Array operations (need Spark 2.4+):
    
    df.withColumn('mapB', expr("filter(split(B, ''), x -> x != '')")) \
      .selectExpr('*', "array_join(transform(split(A,'/'), x -> ifnull(mapB[int(x)],x)), '/') as C") \
      .show()
    +-------+----+------------+-------+
    |      A|   B|        mapB|      C|
    +-------+----+------------+-------+
    |0/1/2/3|  AG|      [A, G]|A/G/2/3|
    |0/1/3/2|RTFS|[R, T, F, S]|R/T/S/F|
    +-------+----+------------+-------+


  Method-2: use translate, only works if length of B is less than 10:
    
    # A: need Spark 2.4+
    df.selectExpr('*', "translate(A, concat_ws('',sequence(0,length(B)-1)), B) as C").show()                              
    +-------+----+-------+
    |      A|   B|      C|
    +-------+----+-------+
    |0/1/2/3|  AG|A/G/2/3|
    |0/1/3/2|RTFS|R/T/S/F|
    +-------+----+-------+
    
    # before Spark 2.4, use the following:
    df.selectExpr('*', "translate(A, substr('0123456789',1,length(B)), B) as C").show()
    +-------+----+-------+
    |      A|   B|      C|
    +-------+----+-------+
    |0/1/2/3|  AG|A/G/2/3|
    |0/1/3/2|RTFS|R/T/S/F|
    +-------+----+-------+



