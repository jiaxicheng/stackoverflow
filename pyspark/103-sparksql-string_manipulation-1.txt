SparkSQL String Manipulation functions:


---
Example-1: mask-replace-inner-part-of-string-column-in-pyspark
  REF:https://stackoverflow.com/questions/58613681/
  Method: retrieving the position of '@' in the string, and then do some math with the Spark SQL functions CONCAT, LEFT, REPEAT and SUBSTR:

    from pyspark.sql.functions import instr, expr

    df = spark.createDataFrame(
            [(e,) for e in ['abc123@gmail.com', '123abc123@yahoo.com', 'abd@gmail.com']]
          , ['email_col']
    ) 

    # set N=2 as a parameter in the SQL expression
    N = 2

    df.withColumn('loc_at', instr('email_col', '@')) \
      .withColumn('new_col', expr("""
            CONCAT(LEFT(email_col,{0}), REPEAT('*', loc_at-2*{0}-1), SUBSTR(email_col, loc_at-{0}))
       """.format(N))).show(truncate=False)
    +-------------------+------+-------------------+
    |email_col          |loc_at|new_col            |
    +-------------------+------+-------------------+
    |abc123@gmail.com   |7     |ab**23@gmail.com   |
    |123abc123@yahoo.com|10    |12*****23@yahoo.com|
    |abd@gmail.com      |4     |abbd@gmail.com     |
    +-------------------+------+-------------------+

  **Note:** to handle email with the length of username less than 5 when N==2, just add
       an IF() statement to the above SQL expression: 

    IF(loc_at < 5
       , CONCAT(LEFT(email_col,1), REPEAT('*', loc_at-3), SUBSTR(email_col, loc_at-1))
       , CONCAT(LEFT(email_col,2), REPEAT('*', loc_at-5), SUBSTR(email_col, loc_at-2))
    )

    # result:
    +-------------------+------+-------------------+
    |email_col          |loc_at|new_col            |
    +-------------------+------+-------------------+
    |abc123@gmail.com   |7     |ab**23@gmail.com   |
    |123abc123@yahoo.com|10    |12*****23@yahoo.com|
    |abd@gmail.com      |4     |a*d@gmail.com      |
    +-------------------+------+-------------------+


  Note: to wrap up the logic into a function:

    from pyspark.sql.functions import expr

    N = 2
    mask_email = lambda x, N: expr("""
    
      IF(INSTR({0}, '@') < {1}*2+2
        , CONCAT(LEFT({0},1), REPEAT('*', INSTR({0}, '@')-2), SUBSTR({0}, INSTR({0}, '@')))
        , CONCAT(LEFT({0},{1}), REPEAT('*', INSTR({0}, '@')-2*{1}-1), SUBSTR({0}, INSTR({0}, '@')-{1})) 
      ) as `{0}_masked`

    """.format(x,N))

    df.select('*', mask_email('email_col', N)).show()
    +-------------------+-------------------+
    |          email_col|   email_col_masked|
    +-------------------+-------------------+
    |   abc123@gmail.com|   ab**23@gmail.com|
    |123abc123@yahoo.com|12*****23@yahoo.com|
    |      abd@gmail.com|      a**@gmail.com|
    +-------------------+-------------------+



Example-2: map values read from one column
  REF: https://stackoverflow.com/questions/60177244/pyspark-udf-hangs-how-to-avoid-udf
  Task: to map values read from one column: for example the original column A = '0/2' and
        map B = 'AGF', we want to convert A into 'A/F'

  The dataframe is as follows:

    df = spark.createDataFrame([('0/1/2/3','AG'),('0/1/3/2','RTFS')],['A','B'])
    
  Method-1: use Array operations (need Spark 2.4+):
    
    df.withColumn('mapB', expr("filter(split(B, ''), x -> x != '')")) \
      .selectExpr('*', "array_join(transform(split(A,'/'), x -> ifnull(mapB[int(x)],x)), '/') as C") \
      .show()
    +-------+----+------------+-------+
    |      A|   B|        mapB|      C|
    +-------+----+------------+-------+
    |0/1/2/3|  AG|      [A, G]|A/G/2/3|
    |0/1/3/2|RTFS|[R, T, F, S]|R/T/S/F|
    +-------+----+------------+-------+


  Method-2: use translate, only works if length of B is less than 10:
    
    # A: need Spark 2.4+
    df.selectExpr('*', "translate(A, concat_ws('',sequence(0,length(B)-1)), B) as C").show()                              
    +-------+----+-------+
    |      A|   B|      C|
    +-------+----+-------+
    |0/1/2/3|  AG|A/G/2/3|
    |0/1/3/2|RTFS|R/T/S/F|
    +-------+----+-------+
    
    # before Spark 2.4, use the following:
    df.selectExpr('*', "translate(A, substr('0123456789',1,length(B)), B) as C").show()
    +-------+----+-------+
    |      A|   B|      C|
    +-------+----+-------+
    |0/1/2/3|  AG|A/G/2/3|
    |0/1/3/2|RTFS|R/T/S/F|
    +-------+----+-------+




Example-3: use regexp_extract to retrieve values of supplied keys and their aliases:
  REF: https://stackoverflow.com/a/64704947/9510729
  Code:

    df = spark.createDataFrame([                                               
       ("otherPartofString Name=<Series VR> Type=<1Ac4> SqVal=<34> conn ID=<2>",),   
       ("otherPartofString ThisName=<Series X> Type=<1B3> SqVal=<34> conn ID=<2> conn Loc=sfo dest=chc bridge otherpartofString..",)
    ],["value"])
    
    keys = ["Name", "Type", "SqVal", "ID", "Loc", "dest"]
    
    # aliases are case-insensitive and added only if exist
    key_aliases = {
        'Type': [ 'ThisType', 'AnyName' ],
        'ID': ['conn ID'],
        'Loc': ['conn Loc']
    }
    
    # set up regex pattern for each key differently
    key_ptns = [ (k, '|'.join([k, *key_aliases[k]]) if k in key_aliases else k) for k in keys ]  
    #[('Name', 'Name'),
    # ('Type', 'Type|ThisType|AnyName'),
    # ('SqVal', 'SqVal'),
    # ('ID', 'ID|conn ID'),
    # ('Loc', 'Loc|conn Loc'),
    # ('dest', 'dest')]  
    
    df.withColumn('value', F.regexp_replace('value','=(\w+)','=<$1>')) \
        .select("*", *[F.regexp_extract('value', r'(?i)\b(?:{0})=<([^>]+)>'.format(p), 1).alias(k) for k,p in key_ptns]) \
        .show()
    +--------------------+---------+----+-----+---+---+----+
    |               value|     Name|Type|SqVal| ID|Loc|dest|
    +--------------------+---------+----+-----+---+---+----+
    |otherPartofString...|Series VR|1Ac4|   34|  2|   |    |
    |otherPartofString...| Series X| 1B3|   34|  2|sfo| chc|
    +--------------------+---------+----+-----+---+---+----+
    
 Notes:
 (1) possibly better to sort aliases by length, in case there are `conn ID`, `Loc ID` for different keys, in such case `ID` should be set after `conn ID` and `Loc ID`

    key_ptns = [ (k, '|'.join(sorted([k]+key_aliases[k],key=len,reverse=True)) if k in key_aliases else k) for k in keys ]

 

