RDD related data manipulation:

Example-1: check and count if consecutive words in a text file start with the same character.
  REF: https://stackoverflow.com/questions/60640211/

    lines = sc.parallelize([
      "Horrid Henry's hound hunts in the massive Murree mountains. While silly stupid Samuel's dark dreadful dragon likes to hunt in skies.", 
      "Horrid Henry's hound and Samuel's dreadful dragon Dany are fast friends and like to hunt and play together. They call themselves fantastic fanciful foursome."
    ])

    lines.flatMap(lambda line: [ 
        (e[i][0],1) for e in [ line.upper().split() ]
          for i in range(0,len(e)-2) if e[i][0] == e[i+1][0] == e[i+2][0] 
     ]).reduceByKey(lambda x,y: x+y).collect()
    #[('H', 3), ('M', 1), ('S', 1), ('D', 2), ('F', 1)]

 Notes:
  (1) use `[ line.upper().split() ]` to convert the line into a list of lists with a single item 
      containing all uppercased words from the current line
  (2) iterate through the item `e` from the above list by `range(0,len(e)-2)` in order to generate 
      all tri-grams of the line
  (3) yield an entry `(e[i][0],1)` only if all three words start with the same character by using 
      `e[i][0] == e[i+1][0] == e[i+2][0]`
  (4) use reduceByKey to do the counting
  (5) operator.itemgetter is slighter better performant than a lambda function, useful in keyBy()
      https://stackoverflow.com/questions/17243620/operator-itemgetter-or-lambda



Example-2: find all mainID which do not shown in the list of secondaryIDs
  REF: https://stackoverflow.com/questions/60609125 [DELETED]

sample text:
---
1: 2 3 4 13
5: 6 7 8 1
9: 10 11 12
13: 14 15 16

    lines = sc.parallelize(['1: 2 3 4 13', '5: 6 7 8 1', '9: 10 11 12', '13: 14 15 16'])

    # rdd1: (mainID, secondaryID)
    # rdd2: (secondaryID, mainID)
    rdd1 = lines.flatMap(lambda x: [ (int(e1[0]), int(e2)) for e1 in [x.split(':')] for e2 in e1[1].split() ] )

    rdd1.collect()                                                                                                     
    #Out[197]: 
    #[(1, 2), (1, 3), (1, 4), (1, 13),
    # (5, 6), (5, 7), (5, 8), (5, 1),
    # (9, 10), (9, 11), (9, 12),
    # (13, 14), (13, 15), (13, 16)]

    rdd2 = rdd1.map(lambda x:(x[1],x[0]))

    # use leftOuterJoin and find the right value x[1][1] is None
    rdd1.leftOuterJoin(rdd2)\
        .filter(lambda x: x[1][1] is None) \
        .keys() \
        .distinct() \
        .collect()                                                                                                         
    # [5, 9]


Example-3: Use flatMap to create pair-RDD:
  REF: https://stackoverflow.com/questions/61289164

  use set to find unique words split from a sentencem run flatMap to create pari-RDD, then run reduceByKey() 
  to count the list of ids exist for each word:

    collection = sc.parallelize([(1, "winter is coming"), (2, "ours is the fury"), (3, "the old the true the brave")])

    collection.flatMap(lambda x: [ (e,[x[0]]) for e in set(x[1].split()) ]).reduceByKey(lambda x,y: x+y).collect()
    #[('coming', [1]),
    # ('winter', [1]),
    # ('is', [1, 2]),
    # ('fury', [2]),
    # ('the', [2, 3]),
    # ('ours', [2]),
    # ('old', [3]),
    # ('brave', [3]),
    # ('true', [3])]



Example-4: zip_longest or map when reduceByKey:
  REF: https://stackoverflow.com/questions/61286521

  use `zip_longest()` and `fillvalue=0` to sum item values of lists by indices:

    from itertools import zip_longest

    rdd = sc.parallelize([ (1,[1,2,3]), (1,[2,3,4]), (1,[1,1])])
    rdd.reduceByKey(lambda x,y: [ sum(z) for z in zip_longest(x,y,fillvalue=0)] ).collect()
    # [(1, [4, 6, 7])]


Example-5: use keyBy + reduceByKey + values to find element with max(count) for each age:
  REF: https://stackoverflow.com/questions/61658487/how-do-i-partition-rank-and-sort-data-using-a-pyspark-rdd

  given rdd1 = (age, code, count), to find the max count for each age using RDD method:

    rdd1 = sc.parallelize([(4, '"388.400000"', 5), (4, '"389.900000"', 2), (13, '"794.310000"', 1), 
      (5, '"995.300000"', 1), (6, '"995.300000"', 1), (4, '"V72.19"', 2), (13, '"V81.2"', 2)]) 

    rdd1.keyBy(lambda x: x[0]).reduceByKey(lambda x,y: x if x[2] >= y[2] else y).values().collect()
    #[(4, '"388.400000"', 5), (13, '"V81.2"', 2), (5, '"995.300000"', 1), (6, '"995.300000"', 1)]

  Notes:
  (1) use keyBy(): to set pair-RDD of (age, (age, code, count))
  (2) reduceByKey(): to find element with max(count) for each age
  (3) use values() to take values only

  The above takes only one item for each key when there are ties at max value (similiar to row_number). to achieve 
  the same result as rank(), see the folloiwng two approaches on saving multiple codes and then take flatMap:

  Method-1: use string concatenate and then split
    rdd1.keyBy(lambda x: x[0]) \
        .reduceByKey(lambda x,y: x if x[2] > y[2] else y if x[2] < y[2] else (x[0], x[1]+','+y[1],x[2])) \
        .flatMap(lambda x:[ (x[0],e,x[1][2]) for e in x[1][1].split(",")]) \
        .collect() 

  Method-2: use list operation
    rdd1.map(lambda x: (x[0],([x[1]],x[2]))) \
        .reduceByKey(lambda x,y: x if x[1] > y[1] else y if x[1] < y[1] else (x[0]+y[0],x[1])) \
        .flatMap(lambda x:[ (x[0],e,x[1][1]) for e in x[1][0]]) \
        .collect()



Example-6: RDD join using pair-RDD setup by keyBy 
  REF: https://stackoverflow.com/questions/58192063/combine-two-different-rdds-with-two-different-sets-of

  pair-RDD are always counted as two-item tuple, in case there are multiple items in the tuple of RDD element,
  only the first two items will get involved. See below example:

    import datetime

    rdd1 = sc.parallelize([
          (u'id2875421', 2, datetime.datetime(2016, 3, 14, 17, 24, 55)
        , datetime.datetime(2016, 3, 14, 17, 32, 30), 1
        , -73.9821548461914, 40.76793670654297, -73.96463012695312, 40.765602111816406
        , u'N', 455)
    ]) 

    rdd2 = sc.parallelize([(u'id2875421', 1.9505895451732258)])

  direct join will preduce the following:

    rdd1.join(rdd2)
    #[(u'id2875421', (2, 1.9505895451732258))]


  To make it work, need to convert rdd1 into a pair RDD, using keyBy() and
  join(rdd2), and then just pick whatever elements to get desired result:
  The function `keyBy(lambda x: x[0])` is a shortcut of `map(lambda x: (x[0], x))`

    from operator import itemgetter

    rdd1.keyBy(itemgetter(0)) \
        .join(rdd2) \
        .map(lambda x: x[1][0][:5] + (x[1][1],) + x[1][0][9:]) \
        .collect()

    #[(u'id2875421',
    #  2,
    #  datetime.datetime(2016, 3, 14, 17, 24, 55),
    #  datetime.datetime(2016, 3, 14, 17, 32, 30),
    #  1,
    #  1.9505895451732258,
    #  u'N',
    #  455)]

