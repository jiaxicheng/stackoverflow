https://stackoverflow.com/questions/56931256/how-to-explode-multiple-columns-different-types-and-different-lengths

Setup
-----

    from pyspark.sql import functions as F, Row

    df = spark.createDataFrame([
          Row(a=1, b=[1, 2, 3, 4, 5, 6], c=['11', '22', '33'], d=['foo'], e=[1,2,3])
        , Row(a=2, b=[], c=[], d=[], e=[111,222])
    ])
        
    >>> df.show()
    +---+------------------+------------+-----+----------+
    |  a|                 b|           c|    d|         e|
    +---+------------------+------------+-----+----------+
    |  1|[1, 2, 3, 4, 5, 6]|[11, 22, 33]|[foo]| [1, 2, 3]|
    |  2|                []|          []|   []|[111, 222]|
    +---+------------------+------------+-----+----------+

    # columns required to explode
    cols = df.columns
 
Method-2: Using SQL higher-order function: [transform][1]
-----------------------------------------------
Use the Spark SQL higher-order function: transform(), do the following:

1. create the following Spark SQL code where **`{0}`** will be replaced by the column_name:

        stmt = '''
            CASE
              WHEN size({0}) in (2,3) THEN
                transform(sequence(0,5), x -> IF((x*size({0}))%6 == 0, {0}[int(x*size({0})/6)], NULL)) 
              ELSE {0}
            END AS {0}
        '''

    **Note:** array transformation only defined when array contains 2 or 3 elements. arrays with other size will be kept as-is. 

3. Run the above SQL with **selectExpr()** for all computed columns

        df1 = df.withColumn('a', F.array('a')) \
                .selectExpr(*[ stmt.format(c) for c in cols ])

        >>> df1.show()
        +---+------------------+---------------+-----+--------------+
        |  a|                 b|              c|    d|             e|
        +---+------------------+---------------+-----+--------------+
        |[1]|[1, 2, 3, 4, 5, 6]|[11,, 22,, 33,]|[foo]|  [1,, 2,, 3,]|
        |[2]|                []|             []|   []|[111,,, 222,,]|
        +---+------------------+---------------+-----+--------------+


4. run **arrays_zip** and **explode**:

        df_new = df1.withColumn('vals', F.explode(F.arrays_zip(*cols))) \
                    .select('vals.*') \
                    .fillna('', subset=cols)

        >>> df_new.show()
        +----+----+---+---+----+
        |   a|   b|  c|  d|   e|
        +----+----+---+---+----+
        |   1|   1| 11|foo|   1|
        |null|   2|   |   |null|
        |null|   3| 22|   |   2|
        |null|   4|   |   |null|
        |null|   5| 33|   |   3|
        |null|   6|   |   |null|
        |   2|null|   |   | 111|
        |null|null|   |   |null|
        |null|null|   |   |null|
        |null|null|   |   | 222|
        |null|null|   |   |null|
        |null|null|   |   |null|
        +----+----+---+---+----+

    **Note**: `fillna('', subset=cols)` only changed columns containing Strings

In one method chain:
--------------------

    df_new = df.withColumn('a', F.array('a')) \
               .selectExpr(*[ stmt.format(c) for c in cols ]) \
               .withColumn('vals', F.explode(F.arrays_zip(*cols))) \
               .select('vals.*') \
               .fillna('', subset=cols)


*** Explanation with the transform function***

The transform function (list below):

    transform(sequence(0,5), x -> IF((x*size({0}))%6 == 0, {0}[int(x*size({0})/6)], NULL))

As mentioned in the post, {0} will be replaced with column name. Here we use column-c which 
contains 3 elements as an example:

* In the transform function, sequence(0,5) yields a constant array `array(0,1,2,3,4,5)` with 6-elements, and the rest are the lambda function with one argument `x` which is the value of elements.

* IF(condition, true_value, false_value): is a standard SQL function

* The condition we applied is: (x*size(c))%6 == 0 where size(c)=3, if this condition is true, it will return c[int(x*size(c)/6)], otherwise, return NULL. so for x from 0 to 5, we will have:

    ((0*3)%6)==0) true   -->  c[int(0*3/6)] = c[0]
    ((1*3)%6)==0) false  -->  NULL
    ((2*3)%6)==0) true   -->  c[int(2*3/6)] = c[1]
    ((3*3)%6)==0) false  -->  NULL
    ((4*3)%6)==0) true   -->  c[int(4*3/6)] = c[2]
    ((5*3)%6)==0) false  -->  NULL

Similar to column-e which contains a 2-element array. In this code, we just re-align 
the arrays when they contain 2 or 3 elements. 

[1]: https://docs.databricks.com/_static/notebooks/apache-spark-2.4-functions.html

