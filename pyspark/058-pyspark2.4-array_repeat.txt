
https://stackoverflow.com/questions/50624745/pyspark-how-to-duplicate-a-row-n-time-in-dataframe

Repeat rows based on a column-value:

With **Spark 2.40+**, this can be handled with builtin functions: **array_repeat** + **explode**:

    from pyspark.sql.functions import expr
    
    df = spark.createDataFrame([(1,2,1), (2,9,1), (3,8,2), (4,1,1), (5,3,3)] ,["A", "B", "n"]) 

    >>> df.show()
    +---+---+---+
    |  A|  B|  n|
    +---+---+---+
    |  1|  2|  1|
    |  2|  9|  1|
    |  3|  8|  2|
    |  4|  1|  1|
    |  5|  3|  3|
    +---+---+---+
    
    new_df = df.withColumn('n', expr('explode(array_repeat(n,int(n)))'))
    
    >>> new_df.show()
    +---+---+---+
    |  A|  B|  n|
    +---+---+---+
    |  1|  2|  1|
    |  2|  9|  1|
    |  3|  8|  2|
    |  3|  8|  2|
    |  4|  1|  1|
    |  5|  3|  3|
    |  5|  3|  3|
    |  5|  3|  3|
    +---+---+---+


