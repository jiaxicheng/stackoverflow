https://stackoverflow.com/questions/57435858/how-do-you-create-merge-asof-functionality-in-pyspark
https://github.com/jiaxicheng/stackoverflow/blob/master/pyspark/065-pyspark-merge_asof.txt

Similar Pandas merge_asof() with pyspark:
Note: The below should be working with the Dataframes matching the following conditions: 
(1) dfA is relatively small but contain many columns
(2) dfB is very huge, no extra field for partitionBy

Similar to the previous problem in the 2nd link above, what we do here:
(1) use a flag `f` to identify data source from dfA(f==1) or dfB(f==0)
(2) find all distinct `date` in dfA, add two columns f==1 and count=None
(3) union the above dataframe to dfB
(4) use Window function last/first to find previous/next non-Null value to 
    complete ffill or ffill based on the merge_asof direction option
(5) after finding all matched count from (4), retrive all entry with f==1
(6) join the result in (5) to the original dfA, this will be a regular join with no 
    unmatched dates

Note: this method will depend on the size of dfB since no partition is available, the Window
function has to be loaded into one partition to execute. that could raise an OOM issue.

Sample Setup:

    from pyspark.sql.functions import col, lit, when, first, last, coalesce, datediff, expr

    dfA.show()
    +---+----------+---+----+----+
    | ID|      date| c1|  c2|  c3|
    +---+----------+---+----+----+
    | 12|2019-01-02| A1|   1|   0|
    | 12|2019-01-05| A2|   2|   1|
    | 12|2019-01-08| A3|   3|   1|
    | 12|2019-01-10| A4|null|   1|
    | 12|2019-01-02| A5|   5|   0|
    | 25|2019-01-03| B1|  11|null|
    | 25|2019-01-06| B2|  12|   0|
    | 25|2019-01-07| B3|  13|   1|
    | 25|2019-01-09| B4|null|   0|
    +---+----------+---+----+----+

    dfB.orderBy('date').show()
    +----------+-----+
    |      date|count|
    +----------+-----+
    |2019-01-01|   80|
    |2019-01-01|   34|
    |2019-01-02|    9|
    |2019-01-03|   70|
    |2019-01-05|   80|
    |2019-01-06|    0|
    |2019-01-08|   24|
    |2019-01-09|    8|
    |2019-01-10|   62|
    |2019-01-12|   64|
    |2019-01-14|   18|
    |2019-01-15|   65|
    |2019-01-16|    3|
    |2019-01-18|   43|
    |2019-01-18|    9|
    |2019-01-19|   64|
    |2019-01-21|    5|
    |2019-01-21|   69|
    |2019-01-22|   94|
    |2019-01-24|   49|
    +----------+-----+

"""For dfA: find distinct date in dfA and add two more columns: 
  + count: initialize to None
  + f    : flag so it's from dfA
"""

    dfA_new = dfA.select('date', lit(None).alias('count'), lit(1).alias('f')).distinct()

    dfA_new.orderBy('date').show()
    +----------+-----+---+                                                          
    |      date|count|  f|
    +----------+-----+---+
    |2019-01-02| null|  1|
    |2019-01-03| null|  1|
    |2019-01-05| null|  1|
    |2019-01-06| null|  1|
    |2019-01-07| null|  1|
    |2019-01-08| null|  1|
    |2019-01-09| null|  1|
    |2019-01-10| null|  1|
    +----------+-----+---+

"""For dfB: add a flag to identify the source of entries
and then union the dfA_new to get a new df
"""

    dfB_new = dfB.withColumn('f', lit(0)).union(dfA_new)
    
    dfB_new.orderBy('date').show()
    +----------+-----+---+                                                          
    |      date|count|  f|
    +----------+-----+---+
    |2019-01-01|   80|  0|
    |2019-01-01|   34|  0|
    |2019-01-02| null|  1|
    |2019-01-02|    9|  0|
    |2019-01-03|   70|  0|
    |2019-01-03| null|  1|
    |2019-01-05|   80|  0|
    |2019-01-05| null|  1|
    |2019-01-06| null|  1|
    |2019-01-06|    0|  0|
    |2019-01-07| null|  1|
    |2019-01-08|   24|  0|
    |2019-01-08| null|  1|
    |2019-01-09|    8|  0|
    |2019-01-09| null|  1|
    |2019-01-10|   62|  0|
    |2019-01-10| null|  1|
    |2019-01-12|   64|  0|
    |2019-01-14|   18|  0|
    |2019-01-15|   65|  0|
    +----------+-----+---+

(1) pd.merge_asof(dfA.sort_values('date'), dfB.sort_values('date'), on='date', direction="forward")

    # bfill (merge_asof direction=forward) in case of tie, put dfA (count=null) first
    w2 = Window.orderBy('date', col('f').desc()).rowsBetween(0,Window.unboundedFollowing)

    dfB_F = dfB_new.withColumn('count', coalesce(col('count'), first('count', ignorenulls=True).over(w2))) \
                   .filter('f==1') \
                   .drop('f')

    then we can use regular join to get the final result:
    dfA_F = dfA.join(dfB_F, on='date', how='left')
    dfA_F.orderBy('date').show()
    +----------+---+---+----+----+-----+                                            
    |      date| ID| c1|  c2|  c3|count|
    +----------+---+---+----+----+-----+
    |2019-01-02| 12| A1|   1|   0|    9|
    |2019-01-02| 12| A5|   5|   0|    9|
    |2019-01-03| 25| B1|  11|null|    6|
    |2019-01-05| 12| A2|   2|   1|    0|
    |2019-01-06| 25| B2|  12|   0|    0|
    |2019-01-07| 25| B3|  13|   1|   24|
    |2019-01-08| 12| A3|   3|   1|   24|
    |2019-01-09| 25| B4|null|   0|    8|
    |2019-01-10| 12| A4|null|   1|   62|
    +----------+---+---+----+----+-----+

    #Pandas merge_asof
    pd.merge_asof(dfA.sort_values('date'), dfB.sort_values('date'), on='date', direction="forward")
       ID       date  c1    c2   c3  count
    0  12 2019-01-02  A1   1.0  0.0      9
    1  12 2019-01-02  A5   5.0  0.0      9
    2  25 2019-01-03  B1  11.0  NaN      6
    3  12 2019-01-05  A2   2.0  1.0      0
    4  25 2019-01-06  B2  12.0  0.0      0
    5  25 2019-01-07  B3  13.0  1.0     24
    6  12 2019-01-08  A3   3.0  1.0     24
    7  25 2019-01-09  B4   NaN  0.0      8
    8  12 2019-01-10  A4   NaN  1.0     62
    
(2) pd.merge_asof(dfA.sort_values('date'), dfB.sort_values('date'), on='date', direction="backward")

    # ffill (merge_asof direction=backward) in case of tie, put dfA_new (count=null) last
    w1 = Window.orderBy('date', 'f').rowsBetween(Window.unboundedPreceding,0)

    dfB_B = dfB_new.withColumn('count', coalesce(col('count'), last('count', ignorenulls=True).over(w1))) \
                   .filter('f==1') \
                   .drop('f')

    dfA_B = dfA.join(dfB_B, on='date', how='left')

    dfA_B.orderBy('date').show()
    +----------+---+---+----+----+-----+                                            
    |      date| ID| c1|  c2|  c3|count|
    +----------+---+---+----+----+-----+
    |2019-01-02| 12| A1|   1|   0|    9|
    |2019-01-02| 12| A5|   5|   0|    9|
    |2019-01-03| 25| B1|  11|null|   10|
    |2019-01-05| 12| A2|   2|   1|   80|
    |2019-01-06| 25| B2|  12|   0|    0|
    |2019-01-07| 25| B3|  13|   1|    0|
    |2019-01-08| 12| A3|   3|   1|   24|
    |2019-01-09| 25| B4|null|   0|    8|
    |2019-01-10| 12| A4|null|   1|    8|
    +----------+---+---+----+----+-----+

(3) pd.merge_asof(dfA.sort_values('date'), dfB.sort_values('date'), on='date', direction="nearest")
 ```
    Fields:
     + date1     : a temporary column with values only when count is NotNull
     + prev_date : the date when the previou count is not NULL in the same partition
     + next_date : the date when the next count is not NULL in the same partition
     + prev_count: the previous notNULL count order by date within the same partition
     + next_count: the next notNULL count order by date within the same partition
     + count     : new count based on the datediff between date and the prev_date/next_date
     + f         : flag for data origin and used in orderBy of WindowSpec for tie break

    dfB_N = dfB_new.withColumn('date1', when(col('count').isNotNull(), col('date'))) \
                   .withColumn('prev_date', last('date1',True).over(w1)) \
                   .withColumn('next_date', first('date1',True).over(w2)) \
                   .withColumn('prev_count', last('count',True).over(w1)) \
                   .withColumn('next_count', first('count',True).over(w2)) \
                   .withColumn('count', coalesce(col('count')
                         , when(datediff(col('date'),col('prev_date')) <= datediff(col('next_date'),col('date'))
                             , col('prev_count')
                           ).otherwise(col('next_count')))) \
                   .filter('f=1') \
                   .drop('date1', 'prev_date', 'next_date', 'prev_count', 'next_count', 'f')

    dfA_N = dfA.join(dfB_N, on='date', how='left')

    dfA_N.orderBy('date').show()
    +----------+---+---+----+----+-----+                                            
    |      date| ID| c1|  c2|  c3|count|
    +----------+---+---+----+----+-----+
    |2019-01-02| 12| A1|   1|   0|    9|
    |2019-01-02| 12| A5|   5|   0|    9|
    |2019-01-03| 25| B1|  11|null|   10|
    |2019-01-05| 12| A2|   2|   1|   80|
    |2019-01-06| 25| B2|  12|   0|    0|
    |2019-01-07| 25| B3|  13|   1|    0|
    |2019-01-08| 12| A3|   3|   1|   24|
    |2019-01-09| 25| B4|null|   0|    8|
    |2019-01-10| 12| A4|null|   1|    8|
    +----------+---+---+----+----+-----+


Notes: 

1. Since no partitionBy() is specified in Window Spec, there will be an WARN message identify potential issue when all data must be loaded into one partition:

> WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.


2. sample date on madison:
dfA = spark.read.csv('file:///home/hdfs/test/pyspark/merge_asof_A.txt', header=True)
dfB = spark.read.csv('file:///home/hdfs/test/pyspark/merge_asof_B_2.txt', header=True)

The below code can be used to create sample dfB:

    dfB = df.selectExpr('shuffle(sequence(to_date("2019-01-01"), to_date("2019-09-19"))) as D') \
            .select(F.explode('D').alias('date')) \
            .sample(fraction=0.3, seed=9) \
            .withColumn('count', (F.rand()*100).astype('integer'))


