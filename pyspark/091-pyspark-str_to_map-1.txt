Using str_to_map:
---


Example-1: 
 REF: https://stackoverflow.com/questions/62024564/split-1-long-txt-column-into-2-columns-in-pysparkdatabricks

    lst = ["PL:1547497782:1547497782~ST:1548593509:1547497782",
      "PU:1547497782:1547497782~MU:1548611698:1547497782:1~MU:1548612195:1547497782:0~ST:1548627786:1547497782",
      "PU:1547497782:1547497782~PU:1547497782:1547497782~ST:1548637508:1547497782",
      "PL:1548631949:0",
      "PL:1548619200:0~PU:1548623089:1548619435~PU:1548629541:1548625887~RE:1548629542:1548625887~PU:1548632702:1548629048~ST:1548635966:1548629048",
      "PL:1548619583:1548619584~ST:1548619610:1548619609",
      "PL:1548619850:0~ST:1548619850:0~PL:1548619850:0~ST:1548619850:0~PL:1548619850:1548619851~ST:1548619856:1548619855"
    ]

    df = spark.createDataFrame([(e,) for e in lst],['event_list'])

    # below for Spark 3.0+ only which does not allow duplicate keys
    spark.conf.set("spark.sql.mapKeyDedupPolicy", "LAST_WIN")

    df.selectExpr("str_to_map(event_list, '~', ':') as map1") \
      .selectExpr(
        "split(map1['PL'],':')[0] as PL", 
        "split(map1['ST'],':')[0] as ST"
    ).show()
    +----------+----------+
    |        PL|        ST|
    +----------+----------+
    |1547497782|1548593509|
    |      null|1548627786|
    |      null|1548637508|
    |1548631949|      null|
    |1548619200|1548635966|
    |1548619583|1548619610|
    |1548619850|1548619850|
    +----------+----------+


 Notes: in case duplicate keys exist, the above method will only retrieve the first key/value pairs, to overcome 
   this issue, need to do the following:

    df1 = df.selectExpr("str_to_map(event_list, '~', ':') as map1", "monotonically_increasing_id() as id")

    df1.selectExpr('id', 'explode(map1)') \
       .groupby('id','key') \
       .agg(F.collect_list('value').alias('value')) \
       .groupby('id') \
       .agg(F.map_from_entries(F.collect_list(F.struct('key','value'))).alias('map2')) \
       .show(10,0)



Example-2: use str_to_map 
  REF: https://stackoverflow.com/questions/62539548

    df = spark.createDataFrame([
       (1,"department=Sales__title=Sales_executive__level=junior"),
       (2,"department=Engineering__title=Software Engineer__level=entry-level")
    ],['person_id','person_attributes']) 

    df.selectExpr("person_id", "explode(str_to_map(person_attributes,'__','=')) as (attribute_key, attribute_value)") \
      .filter('attribute_key != "level"') \
      .show()
    +---------+-------------+-----------------+
    |person_id|attribute_key|  attribute_value|
    +---------+-------------+-----------------+
    |        1|   department|            Sales|
    |        1|        title|  Sales_executive|
    |        2|   department|      Engineering|
    |        2|        title|Software Engineer|
    +---------+-------------+-----------------+



Example-3: use str_to_map to partition values through file paths
  REF: https://stackoverflow.com/questions/63388691

    df = spark.createDataFrame([(e,) for e in 
        ["abc://dev/folder1/date=20200813/id=1", "def://dev/folder25/id=3/date=20200814"]], ["file"])

    df.selectExpr("*","str_to_map(file,'/','=') as m1") \
      .selectExpr("file", "m1['date'] as date", "m1['id'] as id") \
      .show(3,0)
    +-------------------------------------+--------+---+
    |file                                 |date    |id |
    +-------------------------------------+--------+---+
    |abc://dev/folder1/date=20200813/id=1 |20200813|1  |
    |def://dev/folder25/id=3/date=20200814|20200814|3  |
    +-------------------------------------+--------+---+



Example-4: use str_to_map to retrieve only wanted key/values in a StringType column
  REF: https://stackoverflow.com/questions/63504785
  Task: convert `feat` into a Map using SparkSQL str_to_map function, and then concatenate 
        two keys(`1` and `5`) and their values:

    df_new = df.selectExpr("*","str_to_map(feat, ' ', ':') m") \
      .selectExpr("id", "concat_ws(' ', '1:'||m['1'], '5:'||m['5']) as feat")

    df_new.show()
    +---+-------+
    | id|   feat|
    +---+-------+
    | u1|1:a 5:c|
    | u2|    5:b|
    +---+-------+

  Note: `'1:'||m['1']` is the same as `concat('1:', m['1'])`. to use a dynamic list of keys:

    select_idx = [1,5]

    df_new = df.selectExpr("*","str_to_map(feat, ' ', ':') m") \
        .selectExpr("id", f"concat_ws(' ', {','.join(f'{i}||m[{i}]' for i in select_idx)}) as feat")



Example-5: find partition values from paths, similar to Example-3
  REF: https://stackoverflow.com/questions/63579435
  Task: filter last 3 days from data parsed from file paths

    df = sqlContext.createDataFrame([
      (1, '/raw/gsec/qradar/flows/dt=2019-12-01/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (2, '/raw/gsec/qradar/flows/dt=2019-11-30/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (3, '/raw/gsec/qradar/flows/dt=2019-11-29/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (4, '/raw/gsec/qradar/flows/dt=2019-11-28/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (5, '/raw/gsec/qradar/flows/dt=2019-11-27/hour=00/1585218406613_flows_20191201_00.jsonl')
    ], ['id','partition']) 

    df1 = df.selectExpr("*", "str_to_map(partition,'/','=')['dt'] as date")
    +---+----------------------------------------------------------------------------------+----------+
    |id |partition                                                                         |date      |
    +---+----------------------------------------------------------------------------------+----------+
    |1  |/raw/gsec/qradar/flows/dt=2019-12-01/hour=00/1585218406613_flows_20191201_00.jsonl|2019-12-01|
    |2  |/raw/gsec/qradar/flows/dt=2019-11-30/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-30|
    |3  |/raw/gsec/qradar/flows/dt=2019-11-29/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-29|
    |4  |/raw/gsec/qradar/flows/dt=2019-11-28/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-28|
    |5  |/raw/gsec/qradar/flows/dt=2019-11-27/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-27|
    +---+----------------------------------------------------------------------------------+----------+



Example-6: read key/value configuration file with lineSep and then do str_to_map
  REF: https://stackoverflow.com/questions/63705035
  Task: this is useful when parsing configurations files with MultiLine mode
  input file:
<Begin  
Id=1  
Name=John  
Age=32  
<End  

<Begin  
Id=2  
Name=Jack  
Age=20  
<End

    # read the above file using spark.read.text with lineSep='<End'
    df = spark.read.text('/home/xicheng/test/lineSep-1.txt', lineSep='<End')

    cols = ['Id', 'Name', 'Age']

    df.selectExpr("str_to_map(value,'\n','=') as map").selectExpr([f"trim(map['{c}']) as `{c}`" for c in cols]).show()
    +----+----+----+
    |  Id|Name| Age|
    +----+----+----+
    |   1|John|  32|
    |   2|Jack|  20|
    |null|null|null|
    +----+----+----+





