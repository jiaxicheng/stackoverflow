Using str_to_map:
---
Creates a map after splitting the text into key/value pairs using delimiters.

   str_to_map(text[, pairDelim[, keyValueDelim]])  
  
(1) Default delimiters are ',' for pairDelim and ':' for keyValueDelim. 
(2) Both pairDelim and keyValueDelim are treated as regular expressions.


---
Example-1: 
 REF: https://stackoverflow.com/questions/62024564/split-1-long-txt-column-into-2-columns-in-pysparkdatabricks

    lst = ["PL:1547497782:1547497782~ST:1548593509:1547497782",
      "PU:1547497782:1547497782~MU:1548611698:1547497782:1~MU:1548612195:1547497782:0~ST:1548627786:1547497782",
      "PU:1547497782:1547497782~PU:1547497782:1547497782~ST:1548637508:1547497782",
      "PL:1548631949:0",
      "PL:1548619200:0~PU:1548623089:1548619435~PU:1548629541:1548625887~RE:1548629542:1548625887~PU:1548632702:1548629048~ST:1548635966:1548629048",
      "PL:1548619583:1548619584~ST:1548619610:1548619609",
      "PL:1548619850:0~ST:1548619850:0~PL:1548619850:0~ST:1548619850:0~PL:1548619850:1548619851~ST:1548619856:1548619855"
    ]

    df = spark.createDataFrame([(e,) for e in lst],['event_list'])

    # below for Spark 3.0+ only which does not allow duplicate keys
    spark.conf.set("spark.sql.mapKeyDedupPolicy", "LAST_WIN")

    df.selectExpr("str_to_map(event_list, '~', ':') as map1") \
      .selectExpr(
        "split(map1['PL'],':')[0] as PL", 
        "split(map1['ST'],':')[0] as ST"
    ).show()
    +----------+----------+
    |        PL|        ST|
    +----------+----------+
    |1547497782|1548593509|
    |      null|1548627786|
    |      null|1548637508|
    |1548631949|      null|
    |1548619200|1548635966|
    |1548619583|1548619610|
    |1548619850|1548619850|
    +----------+----------+


 Notes: 
  (1) in case duplicate keys exist, the above method will only retrieve the first key/value pairs, to overcome 
      this issue, need to do the following:

    df1 = df.selectExpr("str_to_map(event_list, '~', ':') as map1", "monotonically_increasing_id() as id")

    df1.selectExpr('id', 'explode(map1)') \
       .groupby('id','key') \
       .agg(F.collect_list('value').alias('value')) \
       .groupby('id') \
       .agg(F.map_from_entries(F.collect_list(F.struct('key','value'))).alias('map2')) \
       .show(10,0)

  (2) For Spark 3.0+, duplicate keys yield ERROR, to overcome this, set "spark.sql.mapKeyDedupPolicy" to "LAST_WIN"



Example-2: use str_to_map 
  REF: https://stackoverflow.com/questions/62539548

    df = spark.createDataFrame([
       (1,"department=Sales__title=Sales_executive__level=junior"),
       (2,"department=Engineering__title=Software Engineer__level=entry-level")
    ],['person_id','person_attributes']) 

    df.selectExpr("person_id", "explode(str_to_map(person_attributes,'__','=')) as (attribute_key, attribute_value)") \
      .filter('attribute_key != "level"') \
      .show()
    +---------+-------------+-----------------+
    |person_id|attribute_key|  attribute_value|
    +---------+-------------+-----------------+
    |        1|   department|            Sales|
    |        1|        title|  Sales_executive|
    |        2|   department|      Engineering|
    |        2|        title|Software Engineer|
    +---------+-------------+-----------------+



Example-3: use str_to_map to partition values through file paths
  REF: https://stackoverflow.com/questions/63388691

    df = spark.createDataFrame([(e,) for e in 
        ["abc://dev/folder1/date=20200813/id=1", "def://dev/folder25/id=3/date=20200814"]], ["file"])

    df.selectExpr("*","str_to_map(file,'/','=') as m1") \
      .selectExpr("file", "m1['date'] as date", "m1['id'] as id") \
      .show(3,0)
    +-------------------------------------+--------+---+
    |file                                 |date    |id |
    +-------------------------------------+--------+---+
    |abc://dev/folder1/date=20200813/id=1 |20200813|1  |
    |def://dev/folder25/id=3/date=20200814|20200814|3  |
    +-------------------------------------+--------+---+



Example-4: use str_to_map to retrieve only wanted key/values in a StringType column
  REF: https://stackoverflow.com/questions/63504785
  Task: convert `feat` into a Map using SparkSQL str_to_map function, and then concatenate 
        two keys(`1` and `5`) and their values:

    df_new = df.selectExpr("*","str_to_map(feat, ' ', ':') m") \
      .selectExpr("id", "concat_ws(' ', '1:'||m['1'], '5:'||m['5']) as feat")

    df_new.show()
    +---+-------+
    | id|   feat|
    +---+-------+
    | u1|1:a 5:c|
    | u2|    5:b|
    +---+-------+

  Note: `'1:'||m['1']` is the same as `concat('1:', m['1'])`. to use a dynamic list of keys:

    select_idx = [1,5]

    df_new = df.selectExpr("*","str_to_map(feat, ' ', ':') m") \
        .selectExpr("id", f"concat_ws(' ', {','.join(f'{i}||m[{i}]' for i in select_idx)}) as feat")



Example-5: find partition values from paths, similar to Example-3
  REF: https://stackoverflow.com/questions/63579435
  Task: filter last 3 days from data parsed from file paths

    df = sqlContext.createDataFrame([
      (1, '/raw/gsec/qradar/flows/dt=2019-12-01/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (2, '/raw/gsec/qradar/flows/dt=2019-11-30/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (3, '/raw/gsec/qradar/flows/dt=2019-11-29/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (4, '/raw/gsec/qradar/flows/dt=2019-11-28/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (5, '/raw/gsec/qradar/flows/dt=2019-11-27/hour=00/1585218406613_flows_20191201_00.jsonl')
    ], ['id','partition']) 

    df1 = df.selectExpr("*", "str_to_map(partition,'/','=')['dt'] as date")
    +---+----------------------------------------------------------------------------------+----------+
    |id |partition                                                                         |date      |
    +---+----------------------------------------------------------------------------------+----------+
    |1  |/raw/gsec/qradar/flows/dt=2019-12-01/hour=00/1585218406613_flows_20191201_00.jsonl|2019-12-01|
    |2  |/raw/gsec/qradar/flows/dt=2019-11-30/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-30|
    |3  |/raw/gsec/qradar/flows/dt=2019-11-29/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-29|
    |4  |/raw/gsec/qradar/flows/dt=2019-11-28/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-28|
    |5  |/raw/gsec/qradar/flows/dt=2019-11-27/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-27|
    +---+----------------------------------------------------------------------------------+----------+



Example-6: read key/value configuration file with lineSep and then do str_to_map
  REF: https://stackoverflow.com/questions/63705035
  Task: this is useful when parsing configurations files with MultiLine mode
  input file:
<Begin  
Id=1  
Name=John  
Age=32  
<End  

<Begin  
Id=2  
Name=Jack  
Age=20  
<End

    # read the above file using spark.read.text with lineSep='<End'
    df = spark.read.text('/home/xicheng/test/lineSep-1.txt', lineSep='<End')

    cols = ['Id', 'Name', 'Age']

    df.selectExpr("str_to_map(value,'\n','=') as map").selectExpr([f"trim(map['{c}']) as `{c}`" for c in cols]).show()
    +----+----+----+
    |  Id|Name| Age|
    +----+----+----+
    |   1|John|  32|
    |   2|Jack|  20|
    |null|null|null|
    +----+----+----+



Example-7: set up map based on conditions, use regex pattern for delimiters
  REF: https://stackoverflow.com/questions/63767366/
  Rules:
   (1) do not process if contextMap_ID1 contains the word 'TimeStamp' or it contains AlphaNumeric without ':'
   (2) New Column with 'ReceiptNum' -> contextMap_ID1 if contextMap_ID1 contains NUMERIC 9 digit Number
       or Alpha-NUMERIC starting with 'ABC'
   (3) else: Multiple key value pairs to New Column(s) with key as column Name

Code:

    dfx = sc.parallelize([ 
        ("blah blah blah createdTimeStamp=2020-08-11 15:31:37.458 blah blah blah",) 
      , (123456789,), ("caseId: 2345678 personId: 87654321", ), ("CRON",), ("ABC9876543210",) 
    ]).toDF(["contextMap_ID1"]) 

    sql_expr = """ 
      CASE 
        WHEN contextMap_ID1 RLIKE '^(?:ABC[0-9]+|[0-9]{9})$' THEN map('ReceiptNum', contextMap_ID1) 
        WHEN contextMap_ID1 RLIKE '(?:^(?!.*:)(?=.*[A-Z])|TimeStamp)' THEN NULL 
        ELSE str_to_map(trim(contextMap_ID1),'(?<!:) +', ': *') 
      END as map1 
    """  

    dfx.selectExpr("contextMap_ID1", sql_expr).show(truncate=False)                                                    
    +----------------------------------------------------------------------+-----------------------------------------+
    |contextMap_ID1                                                        |map1                                     |
    +----------------------------------------------------------------------+-----------------------------------------+
    |blah blah blah createdTimeStamp=2020-08-11 15:31:37.458 blah blah blah|null                                     |
    |123456789                                                             |[ReceiptNum -> 123456789]                |
    |caseId: 2345678 personId: 87654321                                    |[caseId -> 2345678, personId -> 87654321]|
    |CRON                                                                  |null                                     |
    |ABC9876543210                                                         |[ReceiptNum -> ABC9876543210]            |
    +----------------------------------------------------------------------+-----------------------------------------+

  An small exercise: if value of map1 is NULL then convert it to map('ReceiptNum', k)

    from pyspark.sql.functions import expr

    df = spark.createDataFrame([ (0, "caseId: 12345 personId: 45678"), (1, "I0E0932846637")
        , (2, "caseId: 12345"), (3, None), (4, "anum: 123 mgr: 1234")], ["id", "contextMapID1"])

    # return logic using two name_struct, the field name must be forced consistent
    sql_expr1 = expr("""
      map_from_entries(transform(map_keys(map1), k -> nvl2(map1[k],(k,map1[k] as v), ('ReceiptNum' as k,k as v))))
    """)

    # return logic packed in one named_struct, the field name can be skipped
    sql_expr2 = expr("""
      map_from_entries(transform(map_keys(map1), k -> (nvl2(map1[k],k,'ReceiptNum'), ifnull(map1[k],k))))
    """)

    df.selectExpr("*", "str_to_map(trim(contextMapID1),'(?<!:) +', ': *') as map1").withColumn('map1', sql_expr1).show(10,0)o
    +---+-----------------------------+------------------------------------+
    |id |contextMapID1                |map1                                |
    +---+-----------------------------+------------------------------------+
    |0  |caseId: 12345 personId: 45678|[caseId -> 12345, personId -> 45678]|
    |1  |I0E0932846637                |[ReceiptNum -> I0E0932846637]       |
    |2  |caseId: 12345                |[caseId -> 12345]                   |
    |3  |null                         |null                                |
    |4  |anum: 123 mgr: 1234          |[anum -> 123, mgr -> 1234]          |
    +---+-----------------------------+------------------------------------+

