Using str_to_map:
---
Creates a map after splitting the text into key/value pairs using delimiters.

   str_to_map(text[, pairDelim[, keyValueDelim]])  
  
(1) Default delimiters are ',' for pairDelim and ':' for keyValueDelim. 
(2) Both pairDelim and keyValueDelim are treated as regular expressions.

Note: this regex delims works even with Spark 1.6.0 (untested for the older Spark versions)

---
Example-1: 
 REF: https://stackoverflow.com/questions/62024564/split-1-long-txt-column-into-2-columns-in-pysparkdatabricks

    lst = ["PL:1547497782:1547497782~ST:1548593509:1547497782",
      "PU:1547497782:1547497782~MU:1548611698:1547497782:1~MU:1548612195:1547497782:0~ST:1548627786:1547497782",
      "PU:1547497782:1547497782~PU:1547497782:1547497782~ST:1548637508:1547497782",
      "PL:1548631949:0",
      "PL:1548619200:0~PU:1548623089:1548619435~PU:1548629541:1548625887~RE:1548629542:1548625887~PU:1548632702:1548629048~ST:1548635966:1548629048",
      "PL:1548619583:1548619584~ST:1548619610:1548619609",
      "PL:1548619850:0~ST:1548619850:0~PL:1548619850:0~ST:1548619850:0~PL:1548619850:1548619851~ST:1548619856:1548619855"
    ]

    df = spark.createDataFrame([(e,) for e in lst],['event_list'])

    # below for Spark 3.0+ only which does not allow duplicate keys
    spark.conf.set("spark.sql.mapKeyDedupPolicy", "LAST_WIN")

    df.selectExpr("str_to_map(event_list, '~', ':') as map1") \
      .selectExpr(
        "split(map1['PL'],':')[0] as PL", 
        "split(map1['ST'],':')[0] as ST"
    ).show()
    +----------+----------+
    |        PL|        ST|
    +----------+----------+
    |1547497782|1548593509|
    |      null|1548627786|
    |      null|1548637508|
    |1548631949|      null|
    |1548619200|1548635966|
    |1548619583|1548619610|
    |1548619850|1548619850|
    +----------+----------+


 Notes: 
  (1) in case duplicate keys exist, the above method will only retrieve the first key/value pairs, to overcome 
      this issue, need to do the following:

    df1 = df.selectExpr("str_to_map(event_list, '~', ':') as map1", "monotonically_increasing_id() as id")

    df1.selectExpr('id', 'explode(map1)') \
       .groupby('id','key') \
       .agg(F.collect_list('value').alias('value')) \
       .groupby('id') \
       .agg(F.map_from_entries(F.collect_list(F.struct('key','value'))).alias('map2')) \
       .show(10,0)

  (2) For Spark 3.0+, duplicate keys yield ERROR, to overcome this, set "spark.sql.mapKeyDedupPolicy" to "LAST_WIN"



Example-2: use str_to_map 
  REF: https://stackoverflow.com/questions/62539548

    df = spark.createDataFrame([
       (1,"department=Sales__title=Sales_executive__level=junior"),
       (2,"department=Engineering__title=Software Engineer__level=entry-level")
    ],['person_id','person_attributes']) 

    df.selectExpr("person_id", "explode(str_to_map(person_attributes,'__','=')) as (attribute_key, attribute_value)") \
      .filter('attribute_key != "level"') \
      .show()
    +---------+-------------+-----------------+
    |person_id|attribute_key|  attribute_value|
    +---------+-------------+-----------------+
    |        1|   department|            Sales|
    |        1|        title|  Sales_executive|
    |        2|   department|      Engineering|
    |        2|        title|Software Engineer|
    +---------+-------------+-----------------+



Example-3: use str_to_map to partition values through file paths
  REF: https://stackoverflow.com/questions/63388691

    df = spark.createDataFrame([(e,) for e in 
        ["abc://dev/folder1/date=20200813/id=1", "def://dev/folder25/id=3/date=20200814"]], ["file"])

    df.selectExpr("*","str_to_map(file,'/','=') as m1") \
      .selectExpr("file", "m1['date'] as date", "m1['id'] as id") \
      .show(3,0)
    +-------------------------------------+--------+---+
    |file                                 |date    |id |
    +-------------------------------------+--------+---+
    |abc://dev/folder1/date=20200813/id=1 |20200813|1  |
    |def://dev/folder25/id=3/date=20200814|20200814|3  |
    +-------------------------------------+--------+---+



Example-4: use str_to_map to retrieve only wanted key/values in a StringType column
  REF: https://stackoverflow.com/questions/63504785
  Task: convert `feat` into a Map using SparkSQL str_to_map function, and then concatenate 
        two keys(`1` and `5`) and their values:

    df_new = df.selectExpr("*","str_to_map(feat, ' ', ':') m") \
      .selectExpr("id", "concat_ws(' ', '1:'||m['1'], '5:'||m['5']) as feat")

    df_new.show()
    +---+-------+
    | id|   feat|
    +---+-------+
    | u1|1:a 5:c|
    | u2|    5:b|
    +---+-------+

  Note: `'1:'||m['1']` is the same as `concat('1:', m['1'])`. to use a dynamic list of keys:

    select_idx = [1,5]

    df_new = df.selectExpr("*","str_to_map(feat, ' ', ':') m") \
        .selectExpr("id", f"concat_ws(' ', {','.join(f'{i}||m[{i}]' for i in select_idx)}) as feat")



Example-5: find partition values from paths, similar to Example-3
  REF: https://stackoverflow.com/questions/63579435
  Task: filter last 3 days from data parsed from file paths

    df = sqlContext.createDataFrame([
      (1, '/raw/gsec/qradar/flows/dt=2019-12-01/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (2, '/raw/gsec/qradar/flows/dt=2019-11-30/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (3, '/raw/gsec/qradar/flows/dt=2019-11-29/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (4, '/raw/gsec/qradar/flows/dt=2019-11-28/hour=00/1585218406613_flows_20191201_00.jsonl'),
      (5, '/raw/gsec/qradar/flows/dt=2019-11-27/hour=00/1585218406613_flows_20191201_00.jsonl')
    ], ['id','partition']) 

    df1 = df.selectExpr("*", "str_to_map(partition,'/','=')['dt'] as date")
    +---+----------------------------------------------------------------------------------+----------+
    |id |partition                                                                         |date      |
    +---+----------------------------------------------------------------------------------+----------+
    |1  |/raw/gsec/qradar/flows/dt=2019-12-01/hour=00/1585218406613_flows_20191201_00.jsonl|2019-12-01|
    |2  |/raw/gsec/qradar/flows/dt=2019-11-30/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-30|
    |3  |/raw/gsec/qradar/flows/dt=2019-11-29/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-29|
    |4  |/raw/gsec/qradar/flows/dt=2019-11-28/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-28|
    |5  |/raw/gsec/qradar/flows/dt=2019-11-27/hour=00/1585218406613_flows_20191201_00.jsonl|2019-11-27|
    +---+----------------------------------------------------------------------------------+----------+



Example-6: read key/value configuration file with lineSep and then do str_to_map
  REF: https://stackoverflow.com/questions/63705035
  Task: this is useful when parsing configurations files with MultiLine mode
  input file:
<Begin  
Id=1  
Name=John  
Age=32  
<End  

<Begin  
Id=2  
Name=Jack  
Age=20  
<End

    # read the above file using spark.read.text with lineSep='<End'
    df = spark.read.text('/home/xicheng/test/lineSep-1.txt', lineSep='<End')

    cols = ['Id', 'Name', 'Age']

    df.selectExpr("str_to_map(value,'\n','=') as map").selectExpr([f"trim(map['{c}']) as `{c}`" for c in cols]).show()
    +----+----+----+
    |  Id|Name| Age|
    +----+----+----+
    |   1|John|  32|
    |   2|Jack|  20|
    |null|null|null|
    +----+----+----+



Example-7: set up map based on conditions, use regex pattern for delimiters
  REF: https://stackoverflow.com/questions/63767366/
  Rules:
   (1) do not process if contextMap_ID1 contains the word 'TimeStamp' or it contains AlphaNumeric without ':'
   (2) New Column with 'ReceiptNum' -> contextMap_ID1 if contextMap_ID1 contains NUMERIC 9 digit Number
       or Alpha-NUMERIC starting with 'ABC'
   (3) else: Multiple key value pairs to New Column(s) with key as column Name

Code:

    dfx = sc.parallelize([ 
        ("blah blah blah createdTimeStamp=2020-08-11 15:31:37.458 blah blah blah",) 
      , (123456789,), ("caseId: 2345678 personId: 87654321", ), ("CRON",), ("ABC9876543210",) 
    ]).toDF(["contextMap_ID1"]) 

    sql_expr = """ 
      CASE 
        WHEN contextMap_ID1 RLIKE '^(?:ABC[0-9]+|[0-9]{9})$' THEN map('ReceiptNum', contextMap_ID1) 
        WHEN contextMap_ID1 RLIKE '(?:^(?!.*:)(?=.*[A-Z])|TimeStamp)' THEN NULL 
        ELSE str_to_map(trim(contextMap_ID1),'(?<!:) +', ': *') 
      END as map1 
    """  

    dfx.selectExpr("contextMap_ID1", sql_expr).show(truncate=False)                                                    
    +----------------------------------------------------------------------+-----------------------------------------+
    |contextMap_ID1                                                        |map1                                     |
    +----------------------------------------------------------------------+-----------------------------------------+
    |blah blah blah createdTimeStamp=2020-08-11 15:31:37.458 blah blah blah|null                                     |
    |123456789                                                             |[ReceiptNum -> 123456789]                |
    |caseId: 2345678 personId: 87654321                                    |[caseId -> 2345678, personId -> 87654321]|
    |CRON                                                                  |null                                     |
    |ABC9876543210                                                         |[ReceiptNum -> ABC9876543210]            |
    +----------------------------------------------------------------------+-----------------------------------------+

  An small exercise: if value of map1 is NULL then convert it to map('ReceiptNum', k)

    from pyspark.sql.functions import expr

    df = spark.createDataFrame([ (0, "caseId: 12345 personId: 45678"), (1, "I0E0932846637")
        , (2, "caseId: 12345"), (3, None), (4, "anum: 123 mgr: 1234")], ["id", "contextMapID1"])

    # return logic using two name_struct, the field name must be forced consistent
    sql_expr1 = expr("""
      map_from_entries(transform(map_keys(map1), k -> nvl2(map1[k],(k,map1[k] as v), ('ReceiptNum' as k,k as v))))
    """)

    # return logic packed in one named_struct, the field name can be skipped
    sql_expr2 = expr("""
      map_from_entries(transform(map_keys(map1), k -> (nvl2(map1[k],k,'ReceiptNum'), ifnull(map1[k],k))))
    """)

    df.selectExpr("*", "str_to_map(trim(contextMapID1),'(?<!:) +', ': *') as map1").withColumn('map1', sql_expr1).show(10,0)o
    +---+-----------------------------+------------------------------------+
    |id |contextMapID1                |map1                                |
    +---+-----------------------------+------------------------------------+
    |0  |caseId: 12345 personId: 45678|[caseId -> 12345, personId -> 45678]|
    |1  |I0E0932846637                |[ReceiptNum -> I0E0932846637]       |
    |2  |caseId: 12345                |[caseId -> 12345]                   |
    |3  |null                         |null                                |
    |4  |anum: 123 mgr: 1234          |[anum -> 123, mgr -> 1234]          |
    +---+-----------------------------+------------------------------------+



Example-8: extract values from String with specific patterns
  REF: https://stackoverflow.com/questions/64473130
  Method: using str_to_map to split String into key/value pairs and then retrieve values from desired keys:
     for str_to_map, we have pairDelim=`> *` and keyValueDelim=`=<`

    from pyspark.sql import functions as F

    df = spark.createDataFrame([ 
      ("Type=<Series VR> Model=<1Ac4> ID=<34> conn seq=<2>",),
      ("Type=<SeriesX> Model=<12Q3> ID=<231> conn seq=<3423123>",)
    ], ['value'])

    keys = ['Type', 'Model', 'ID', 'conn seq']

    df.selectExpr("str_to_map(value, '> *', '=<') as m") \
        .select([ F.col('m')[k].alias(k) for k in keys ]) \
        .show()
    +---------+-----+---+--------+
    |     Type|Model| ID|conn seq|
    +---------+-----+---+--------+
    |Series VR| 1Ac4| 34|       2|
    |  SeriesX| 12Q3|231| 3423123|
    +---------+-----+---+--------+

  Notes: 
   (1) To find all keys in a MapType column: https://stackoverflow.com/a/40603522/9510729

   (2) To make the search case-insensitive, do the following:

     + for Spark 3.0+
      use transform_keys to convert map keys to lowercase, and then use `F.col('m')[k.lower()]` to 
      access the corresponding values

    df_new = df.withColumn("m", F.expr("str_to_map(value, '> *', '=<')")) \
        .withColumn("m", F.expr("transform_keys(m, (k,v) -> lower(k))")) \
        .select([ F.col('m')[k.lower()].alias(k) for k in keys ])

     + for Spark 2.4.*, use the following to replace transform_keys()

      map_from_entries(transform(map_keys(m), k -> (lower(k), m[k])))

     + for Spark 2.3.*, use pandas_udf

      ptn = "(?i)\\b({})(?==)".format('|'.join(keys))
      lower_keys = F.pandas_udf(lambda s: s.str.replace(ptn, lambda m: m.group(1).lower()), "string")

      df_new = df.withColumn('value', lower_keys('value')) \
          .withColumn("m", F.expr("str_to_map(value, '> *', '=<')")) \
          .select([ F.col('m')[k.lower()].alias(k) for k in keys ])


  Example-8-extra: a little bit more complex than the above, where pairs are delimited by SPACE
     which also exists in values and/or keys. thus we use lookahead anchor to set only SPACE 
     followed by one of the keys

    df = spark.createDataFrame([('Type=Series VRsomeTest Model=1Ac4 ID=4 sometesthere',)], ['value'])

    df.withColumn("m", F.expr("str_to_map(value,'(?i) +(?=type|model|id|conn seq)','=')")).show(1,0)
    +---------------------------------------------------+----------------------------------------------------------------+
    |value                                              |m                                                               |
    +---------------------------------------------------+----------------------------------------------------------------+
    |Type=Series VRsomeTest Model=1Ac4 ID=4 sometesthere|[Type -> Series VRsomeTest, Model -> 1Ac4, ID -> 4 sometesthere]|
    +---------------------------------------------------+----------------------------------------------------------------+


Example-8-2: similar to above using str_to_map:
    REF: https://stackoverflow.com/questions/50579452

    df = spark.createDataFrame([
      ('name:Pradnya,IP:100.0.0.4, college: SDM, year:2018',),
      ('name:Ram, IP:100.10.10.5, college: BVB, semester:IV, year:2018',)
    ], ['_c0'])

    df.withColumn('id', F.monotonically_increasing_id()) \
        .selectExpr("id", "explode(str_to_map(_c0,', *',':'))") \
        .groupby("id") \
        .pivot('key') \
        .agg(F.first('value')) \
        .show()
    +---+-----------+-------+-------+--------+----+                                 
    | id|         IP|college|   name|semester|year|
    +---+-----------+-------+-------+--------+----+
    |  0|  100.0.0.4|    SDM|Pradnya|    null|2018|
    |  1|100.10.10.5|    BVB|    Ram|      IV|2018|
    +---+-----------+-------+-------+--------+----+


Example-8-3: create Map, using rtrim to remove potential unwanted chars.
  REF: https://stackoverflow.com/q/65307143/9510729
  Method: adding `rtrim` is better than using pariDelim=`"\\s*\\||"$` which yields an extra EMPTY key, see below `Note`

    text="""ABC:"MobileData"|XYZ:"TableData"|ZXC:"MacData"|MNB:"WindowData"
ABC:"value1"    |XYZ:"value2"   |ZXC:"value3" |MNB:"value4"
ABC: "valueA"   |XYZ:"ValueB"   |ZXC:"valueC" |MNB:"valueD"|POI:"valueE"
ABC:"value11"    |XYZ:"value12"   |ZXC:"value13" |MNB:"value14"
ABC:"value1A"    |XYZ:"value2A"   |ZXC:"value3A"
    """

    df = spark.read.csv(spark.sparkContext.parallelize(text.split('\n')), sep='\n').toDF('value')

    df.selectExpr(r"""str_to_map(rtrim('"',value),'"\\s*\\|', ':\\s*"') as m""").show(truncate=False)
    +---------------------------------------------------------------------------+
    |m                                                                          |
    +---------------------------------------------------------------------------+
    |[ABC -> MobileData, XYZ -> TableData, ZXC -> MacData, MNB -> WindowData]   |
    |[ABC -> value1, XYZ -> value2, ZXC -> value3, MNB -> value4]               |
    |[ABC -> valueA, XYZ -> ValueB, ZXC -> valueC, MNB -> valueD, POI -> valueE]|
    |[ABC -> value11, XYZ -> value12, ZXC -> value13, MNB -> value14]           |
    |[ABC -> value1A, XYZ -> value2A, ZXC -> value3A]                           |
    +---------------------------------------------------------------------------+

  Note: this can also be handled with rtrim:

    df.selectExpr(r"""str_to_map(value,'"\\s*\\||"$', ':\\s*"') as m""").show(truncate=False)
    +--------------------------------------------------------------------------------+
    |m                                                                               |
    +--------------------------------------------------------------------------------+
    |[ABC -> MobileData, XYZ -> TableData, ZXC -> MacData, MNB -> WindowData,  ->]   |
    |[ABC -> value1, XYZ -> value2, ZXC -> value3, MNB -> value4,  ->]               |
    |[ABC -> valueA, XYZ -> ValueB, ZXC -> valueC, MNB -> valueD, POI -> valueE,  ->]|
    |[ABC -> value11, XYZ -> value12, ZXC -> value13, MNB -> value14,  ->]           |
    |[ABC -> value1A, XYZ -> value2A, ZXC -> value3A,  ->]                           |
    +--------------------------------------------------------------------------------+

    
