https://stackoverflow.com/questions/57420752/spark-iterating-through-columns-in-each-row-to-create-a-new-dataframe

Data Setup    

    from pyspark.sql import functions as F    

    df.show()
    +---------+---------+---------+---------+----------+
    |     ColA|     ColB|     ColC|     ColD|      ColE|
    +---------+---------+---------+---------+----------+
    |     null|sample_1x|sample_1y|     null| sample_1z|
    |sample2_x|sample2_y|     null|     null|      null|
    |sample3_x|     null|     null|     null| sample3_y|
    |sample4_x|sample4_y|     null|sample4_z|sample4_zz|
    |sample5_x|     null|     null|     null|      null|
    +---------+---------+---------+---------+----------+
    
    cols = df.columns
    
    df.select(F.array(cols).alias('arr')).show(10,0)
    +----------------------------------------------+
    |arr                                           |
    +----------------------------------------------+
    |[, sample_1x, sample_1y,, sample_1z]          |
    |[sample2_x, sample2_y,,,]                     |
    |[sample3_x,,,, sample3_y]                     |
    |[sample4_x, sample4_y,, sample4_z, sample4_zz]|
    |[sample5_x,,,,]                               |
    +----------------------------------------------+

#################
Method-1: use UDF 
    
(1) Use udf and export array of arrays:

def find_route(arr, cols):
    d = [ (cols[i],e) for i,e in enumerate(arr) if e is not None ]
    return [ (d[i][1], d[i+1][1], d[i][0]+'_'+d[i+1][0]) for i in range(len(d)-1) ]

udf_find_route = F.udf(lambda a: find_route(a, cols), 'array<array<string>>')
    
fields = ['From', 'To', 'Label']    

df.select(F.explode(udf_find_route(F.array(cols))).alias('c1')) \
  .select([ F.col('c1')[i].alias(fields[i]) for i in range(len(fields)) ]) \
  .show()
    
    
(2) Use F.udf and export array of structs:

  method-1: use closure:

    # defind function to convert array into array of structs
    def find_route(arr, cols):
        d = [ (cols[i],e) for i,e in enumerate(arr) if e is not None ]
        return [ {'From':d[i][1], 'To':d[i+1][1], 'Label':d[i][0]+'_'+d[i+1][0]} for i in range(len(d)-1) ]
    
    # set up the UDF and add cols as an extra argument
    udf_find_route = F.udf(lambda a: find_route(a, cols), 'array<struct<From:string,To:string,Label:string>>')

    # retrive the data from the array of structs
    df.select(F.explode(udf_find_route(F.array(cols))).alias('c1')).select('c1.*').show()
    +---------+----------+---------+
    |     From|        To|    Label|
    +---------+----------+---------+
    |sample_1x| sample_1y|ColB_ColC|
    |sample_1y| sample_1z|ColC_ColE|
    |sample2_x| sample2_y|ColA_ColB|
    |sample3_x| sample3_y|ColA_ColE|
    |sample4_x| sample4_y|ColA_ColB|
    |sample4_y| sample4_z|ColB_ColD|
    |sample4_z|sample4_zz|ColD_ColE|
    +---------+----------+---------+

  method-2: use literal column

    @F.udf("array<struct<From:string,To:string,Label:string>>")
    def find_route(arr, cols):
        d = [ (cols[i],e) for i,e in enumerate(arr) if e is not None ]
        return [ {'From':d[i][1], 'To':d[i+1][1], 'Label':d[i][0]+'_'+d[i+1][0]} for i in range(len(d)-1) ]

    list_cols = F.array(*[F.lit(c) for c in cols])

    df.select(F.explode(find_route(F.array(cols), list_cols)).alias('c1')).select('c1.*').show()
    

#################################
Method-2: Only for Spark 2.4.0+, use SparkSQL builtin functions:

    cols = df.columns

    """
      1. arr     : create an array column to save all columns
      2. arr_cols: set up an array of column names
      3. arr_idx : get index of all non-null elements in array `arr`
      4. arr_val : get the corresponding array values from the above index
      5. arr_col : get the correpsonding column names from the above index
      6. arr_new : transform the `arr` with non-null elements into array of structs 
                   which contains `From`, `To` and `Label` fields
      7. select arr_new and dropna when there is only one non-null element in the original array
    """
    df.withColumn('arr', F.array(cols)) \
      .withColumn('arr_cols', F.array(*[F.lit(c) for c in cols])) \
      .withColumn('arr_idx', F.expr('filter(sequence(0,size(arr)-1), i -> arr[i] is not NULL)')) \
      .withColumn('arr_val', F.expr('transform(arr_idx, i -> arr[i])')) \
      .withColumn('arr_col', F.expr('transform(arr_idx, i -> arr_cols[i])')) \
      .withColumn('arr_new', F.explode(F.expr('''
            transform(sequence(0,size(arr_idx)-2), i -> 
                  named_struct('From', arr_val[i], 'To', arr_val[i+1], 'Label', concat(arr_col[i], '_', arr_col[i+1]))
            )
       '''))) \
      .select('arr_new.*') \
      .dropna(subset=['Label']) \
      .show()
    +---------+----------+---------+
    |     From|        To|    Label|
    +---------+----------+---------+
    |sample_1x| sample_1y|ColB_ColC|
    |sample_1y| sample_1z|ColC_ColE|
    |sample2_x| sample2_y|ColA_ColB|
    |sample3_x| sample3_y|ColA_ColE|
    |sample4_x| sample4_y|ColA_ColB|
    |sample4_y| sample4_z|ColB_ColD|
    |sample4_z|sample4_zz|ColD_ColE|
    +---------+----------+---------+
    
##############################
Sample data on madison server: 
  df = spark.read.csv('file:///home/hdfs/test/pyspark/array-3.txt', header=True)

