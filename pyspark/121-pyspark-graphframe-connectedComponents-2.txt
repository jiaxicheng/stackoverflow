More example using connectedComponents method from GraphFrame:

Example-1: simple-one, two vertices of the edges are homogeneous, the same type:

   REF: https://stackoverflow.com/questions/57938223/group-by-certain-record-in-array-pyspark

    
    from graphframes import GraphFrame
    from pyspark.sql.functions import collect_set
    
    df = spark.createDataFrame([
          ('a', list('abcdefghi')),('b', list('bcdejk')), ('c', list('cflm')), ('d', list('kbd')), ('n', list('nopq'))
        , ('p', list('prst')), ('u', list('uvwx')), ('b', list('bfeg')), ('c', list('cbgh')), ('a', list('alfm'))
        ], ['name', 'data']) 
    
    spark.sparkContext.setCheckpointDir("/tmp/111")
    
    edges = df.selectExpr('name as src', 'explode(data) as dst')
    vertices = edges.selectExpr('src as id').union(edges.select('dst')).distinct()
    
    g = GraphFrame(vertices, edges)

    df1 = g.connectedComponents().groupby('component').agg(collect_set('id').alias('ids'))
    df1.show(truncate=False)
    +----------+---------------------------------------+
    |component |ids                                    |
    +----------+---------------------------------------+
    |3         |[r, t, n, s, p, q, o]                  |
    |8589934596|[u, v, x, w]                           |
    |0         |[c, l, k, h, i, m, b, g, j, a, e, f, d]|
    +----------+---------------------------------------+


Example-2: vertices can from different column/sources and are not related
   REF: https://stackoverflow.com/questions/57917487/group-row-by-item-in-arraytype-column-using-pyspark 
   similar: https://stackoverflow.com/questions/59034395

    df = spark.createDataFrame([
        (1, 'Hanoi', ['012346789', '032145698', '0565622253']),
        (2, 'Singapore', ['012346789', '069855633']),
        (3, 'Moscow', ['023466466', '069855633']),
        (4, 'Tokyo', ['044656611', '061316561'])
    ], ['id', 'address', 'phone_list'])

    edges = df.selectExpr('address as src', 'explode(phone_list) as dst')
    vertices = edges.selectExpr('src as id').union(edges.select('dst')).distinct()

    g = GraphFrame(vertices, edges)

    spark.sparkContext.setCheckpointDir("/tmp/111")
    df1 = g.connectedComponents().groupby('component').agg(collect_set('id').alias('ids'))
    df1.show(truncate=False)
    +---------+----------------------------------------------------------------------------------+
    |component|ids                                                                               |
    +---------+----------------------------------------------------------------------------------+
    |0        |[023466466, Moscow, 012346789, 069855633, Hanoi, Singapore, 0565622253, 032145698]|
    |2        |[044656611, 061316561, Tokyo]                                                     |
    +---------+----------------------------------------------------------------------------------+

    df1.selectExpr(
          'component as id'
        , 'filter(ids, x -> x not rlike "^[0-9]+$") AS address'
        , 'filter(ids, x -> x rlike "^[0-9]+$") AS phone_list'
    ).show(truncate=False)
    +---+--------------------------+--------------------------------------------------------+
    |id |address                   |phone_list                                              |
    +---+--------------------------+--------------------------------------------------------+
    |0  |[Moscow, Hanoi, Singapore]|[023466466, 012346789, 069855633, 0565622253, 032145698]|
    |2  |[Tokyo]                   |[044656611, 061316561]                                  |
    +---+--------------------------+--------------------------------------------------------+


Example-3: Use GraphFrame.connectedComponents to find the root_id(parent_id is NULL) of any specific `id`s:
  REF: https://stackoverflow.com/questions/61925975/spark-sql-best-way-to-programmatically-loop-over-a-table
    
    df = spark.createDataFrame([(1,None),(2,1),(3,1),(4,None),(5,4),(6,None),(7,6),(8,3)], ['Node_id','Parent_id'])
    
    from graphframes import GraphFrame 
    
    spark.sparkContext.setCheckpointDir("/tmp/111")
    # below used when we use the default algorithm="graphframes" with g.connectedComponents()
    #spark.conf.set("spark.sql.shuffle.partitions", 10)
    
    v = df.toDF('id', 'parent_id')
    e = df.selectExpr('Node_id as src', 'Parent_id as dst').filter('Parent_id is not null')
    g = GraphFrame(v,e)
    
    # algorithm='graphx' is faster than the default `graphframes` 
    result = g.connectedComponents(algorithm='graphx')
    result.orderBy('id').show()                                                                                         
    +---+---------+---------+                                                       
    | id|parent_id|component|
    +---+---------+---------+
    |  1|     null|        1|
    |  2|        1|        1|
    |  3|        1|        1|
    |  4|     null|        4|
    |  5|        4|        4|
    |  6|     null|        6|
    |  7|        6|        6|
    |  8|        3|        1|
    +---+---------+---------+

    result.join(
        result.selectExpr('component', 'id as root_id').filter('parent_id is null'), 
        'component'
    ).filter('id in (7,8)') \
    .select('id', 'root_id') \
    .show() 
    +---+-------+                                                                   
    | id|root_id|
    +---+-------+
    |  7|      6|
    |  8|      1|
    +---+-------+



Example-4: use connectedComponents to find start and end nodes of any independant chain (remove intermediate nodes)
  REF: https://stackoverflow.com/questions/63826536/how-build-parent-child-relationship-in-pyspark-or-python
  Method: 
    (1) create Graph based on the given df and find g.connectedComponents and group ids for the same `component`
    (2) join with edges using `arrays_overlap(ids, array(src,dst))`, so we find related edges all in the same `component`
    (3) for the same component, find all nodes in `srcs` and `dsts`
    (4) use array_except to find nodes from ids which is in srcs but not in dsts
    (5) use two `transform` to get all  of permutations from srcs to dsts
  Note: the values of `df1.component` has nothing to do with the values of edges.src or edges.dst

  Code:

    from graphframes import GraphFrame
    from pyspark.sql.functions import collect_set, first, expr

    df = spark.createDataFrame([(20,2),(1,2),(3,4),(5,6) ,(7,8),(9,10),(2,11),(4,12),(6,13),(8,14),(14,19)],['key', 'value'])

    spark.sparkContext.setCheckpointDir("/tmp/111")
    spark.conf.set('spark.sql.shuffle.partitions', 5)

    # setup graph
    edges = df.toDF('src','dst')
    vertices = edges.selectExpr('src as id').distinct().union(edges.select('dst').distinct())
    g = GraphFrame(vertices, edges)
    
    df1 = g.connectedComponents()
    
    df1.groupby('component') \
       .agg(collect_set('id').alias('ids')) \
       .join(edges, expr("arrays_overlap(ids, array(src,dst))")) \
       .groupby('component') \
       .agg(
           first('ids').alias('ids'), 
           collect_set('src').alias('srcs'),
           collect_set('dst').alias('dsts')
        ).selectExpr("array_except(ids, dsts) as srcs", "array_except(ids, srcs) as dsts") \
       .selectExpr("inline(flatten(transform(srcs, x -> transform(dsts, y -> (x as src, y as dst)))))") \
       .show()
    +---+---+
    |src|dst|
    +---+---+
    |  7| 19|
    |  3| 12|
    |  5| 13|
    |  1| 11|
    | 20| 11|
    |  9| 10|
    +---+---+

