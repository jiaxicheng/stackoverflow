https://stackoverflow.com/questions/59634932/issue-with-pyspark-dataframe-with-redundant-values

Use groupby + pivot
---
(1) pivot to use a list (customize order of columns)
(2) convert month from number to Str: `date_format(to_date(concat(2020,MONTH), 'yyyyMM'),'MMM')`

    from pyspark.sql.functions import expr, first

    df = spark.read.csv('/home/xicheng/test/pivot-3.txt', header=True)
    +-----------+----+-----+------+-------------------+
    |CLIENT_NAME|YEAR|MONTH|ENGINE|TOTAL_UNIQUE_MEMBER|
    +-----------+----+-----+------+-------------------+
    |       Paax|2019|   12|  ERG2|             435911|
    |       Paax|2019|   11|   ELE|             435911|
    |       Paax|2019|   11|   PHA|             435911|
    |       Paax|2019|   12|   ELE|             435911|
    |       Paax|2019|   12|   EBM|             512518|
    |       Paax|2019|   12|   PHA|             435911|
    +-----------+----+-----+------+-------------------+

    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

    df_new = (df.withColumn('month', expr("date_format(to_date(concat(2020,MONTH), 'yyyyMM'),'MMM')"))
        .groupby('ENGINE')
        .pivot('month', months)
        .agg(first('TOTAL_UNIQUE_MEMBER'))
        .fillna(''))

    df_new.show()
    +------+---+---+---+---+---+---+---+---+---+---+------+------+                  
    |ENGINE|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|   Nov|   Dec|
    +------+---+---+---+---+---+---+---+---+---+---+------+------+
    |   PHA|   |   |   |   |   |   |   |   |   |   |435911|435911|
    |  ERG2|   |   |   |   |   |   |   |   |   |   |      |435911|
    |   ELE|   |   |   |   |   |   |   |   |   |   |435911|435911|
    |   EBM|   |   |   |   |   |   |   |   |   |   |      |512518|
    +------+---+---+---+---+---+---+---+---+---+---+------+------+


