https://stackoverflow.com/questions/59620636/generate-a-report-of-mismatch-columns-between-2-pyspark-dataframes

Find mismatches from two dataframe with the same schema (outer join by the key column):

    from pyspark.sql.functions import when, array, count, first

    df1 = spark.read.csv('/home/xicheng/test/match-1.txt', header=True)
    df2 = spark.read.csv('/home/xicheng/test/match-1-1.txt', header=True)
    
    # list of columns to be compared
    cols = df1.columns[1:]
    
    df_new = (df1.join(df2, on=["key"], how='full') 
        .select([ when(~df1[c].eqNullSafe(df2[c]), array(df1[c], df2[c])).alias(c) for c in cols ]) 
        .selectExpr('stack({},{}) as (colName, mismatch)'.format(len(cols), ','.join('"{0}",`{0}`'.format(c) for c in cols)))
        .filter('mismatch is not NULL'))
    
    df_new.show(10)                                                                                                    
    +-------+-----------+
    |colName|   mismatch|
    +-------+-----------+
    |   col4|  [ab, abx]|
    |   col6|[def, defg]|
    |   col6|[def, defg]|
    |   col5|[abc, abcd]|
    |   col6|[def, defg]|
    |   col1|    [, 123]|
    |   col2|    [, xyz]|
    |   col3|      [, a]|
    |   col4|     [, ab]|
    |   col5|    [, abc]|
    +-------+-----------+
    only showing top 10 rows

**Notes:** 
(1) the condition `~df1[c].eqNullSafe(df2[c])` used to find the mismatches satisfies either of the following:

    + df1[c] != df2[c]
    + df1[c] is NULL or df2[c] is NULL but not both

(2) The mismatches if exist are saved as *ArrayType* column with the first item from *df1* and 2nd item from *df2*. NULL is returned if no mismatch and later filtered out.

(3) the *stack()* function dynamically generated by Python format functions is as follows:

    stack(8,"col1",`col1`,"col2",`col2`,"col3",`col3`,"col4",`col4`,"col5",`col5`,"col6",`col6`,"col7",`col7`,"col8",`col8`) as (colName, mismatch)

After we have df_new, then we can do the groupby + aggregation

    df_new.groupby('colName') \
        .agg(count('mismatch').alias('NumOfMismatch'), first('mismatch').alias('mismatch')) \
        .selectExpr('colName', 'NumOfMismatch', 'mismatch[0] as misMatchFromdf1', 'mismatch[1] as misMatchFromdf2')
        .show()
    +-------+-------------+---------------+---------------+                         
    |colName|NumOfMismatch|misMatchFromdf1|misMatchFromdf2|
    +-------+-------------+---------------+---------------+
    |   col8|            2|           null|            uvw|
    |   col3|            2|           null|              a|
    |   col4|            3|             ab|            abx|
    |   col1|            2|           null|            123|
    |   col6|            5|            def|           defg|
    |   col5|            3|            abc|           abcd|
    |   col2|            2|           null|            xyz|
    |   col7|            2|           null|            qew|
    +-------+-------------+---------------+---------------+


Method-2: using reduce function to setup df_new

This is much slower than using stack(), for 2 columns, Spark has to scan two files 8 times, so there are
16 `FileScan csv` from df_new.explain(), while method using stack() only scan each of two files once.

    from functools import reduce

    df3 = df1.join(df2, on=["key"], how='full')

    df_new = reduce(lambda d1, d2: d1.union(d2), [ 
         df3.select("key", when(~df1[c].eqNullSafe(df2[c]), array(df1[c], df2[c])).alias('mismatch')) \
            .filter('mismatch is not NULL') for c in cols ])



