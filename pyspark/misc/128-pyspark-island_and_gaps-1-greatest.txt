Gaps & Islands:
---


Example-1: use Window function + greatest function
  REF: https://stackoverflow.com/questions/62513690/find-total-view-time

Sample data:

    df = spark.createDataFrame([
              ('101', 'Movie1', 1, 10) 
            , ('101', 'Movie1', 5, 15) 
            , ('101', 'Movie1', 6, 12) 
            , ('101', 'Movie1', 18, 23) 
            , ('102', 'Movie1', 1, 10) 
            , ('102', 'Movie1', 5, 15) 
            , ('102', 'Movie1', 18, 23) 
          ], ('user_id', 'episode_name', 'start_time', 'end_time')
    ) 
    
Method-1: using Window function + greatest:

    from pyspark.sql import functions as F, Window

    w1 = Window.partitionBy('user_id','episode_name').orderBy('start_time').rowsBetween(Window.unboundedPreceding, -1)

    df1 = df.withColumn('prev_max_end', F.max('end_time').over(w1)) \
        .withColumn('effective_hours', F.greatest('prev_max_end','end_time') - F.greatest('prev_max_end','start_time'))

    df1.show()
    +-------+------------+----------+--------+------------+---------------+                    
    |user_id|episode_name|start_time|end_time|prev_max_end|effective_hours|
    +-------+------------+----------+--------+------------+---------------+                    
    |    101|      Movie1|         1|      10|        null|              9|
    |    101|      Movie1|         5|      15|          10|              5|
    |    101|      Movie1|         6|      12|          15|              0|
    |    101|      Movie1|        18|      23|          15|              5|
    |    102|      Movie1|         1|      10|        null|              9|
    |    102|      Movie1|         5|      15|          10|              5|
    |    102|      Movie1|        18|      23|          15|              5|
    +-------+------------+----------+--------+------------+---------------+                    

    df1.groupby('user_id','episode_name').agg(F.sum('duration').alias('total_hours')).show()                            
    +-------+------------+-----------+                                              
    |user_id|episode_name|total_hours|
    +-------+------------+-----------+
    |    101|      Movie1|         19|
    |    102|      Movie1|         19|
    +-------+------------+-----------+

The same method using Spark SQL:

    df.createOrReplaceTempView('tb')

    spark.sql("""

        with t1 as (
          SELECT *, max(end_time) OVER (
            PARTITION BY user_id, episode_name 
            ORDER BY start_time
            ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING
          ) as prev_max_end 
          FROM tb
        )
        SELECT user_id
        , episode_name
        , sum(greatest(prev_max_end,end_time) - greatest(prev_max_end,start_time)) as total_hours
        FROM t1
        GROUP BY user_id, episode_name

    """).show()


Method-2: using aggregate function: Spark 2.4+

    from pyspark.sql import functions as F
    
    df.groupby('user_id','episode_name') \
      .agg(F.sort_array(F.collect_list(F.struct('start_time','end_time'))).alias('dta')) \
      .selectExpr("*","""
        aggregate(
          dta,
          (0L as total_hours, 0L as last_end), 
          (acc, y) -> named_struct(
              'total_hours', acc.total_hours + greatest(y.end_time,acc.last_end) - greatest(y.start_time,acc.last_end), 
              'last_end', greatest(y.end_time,acc.last_end)
            ), 
          acc -> acc.total_hours
        ) as total_hours
      """).show(truncate=False)
    +-------+------------+-------------------------------------+-----------+        
    |user_id|episode_name|dta                                  |total_hours|
    +-------+------------+-------------------------------------+-----------+
    |101    |Movie1      |[[1, 10], [5, 15], [6, 12], [18, 23]]|19         |
    |102    |Movie1      |[[1, 10], [5, 15], [18, 23]]         |19         |
    +-------+------------+-------------------------------------+-----------+
    


Example-2: using Window aggregate function and greatest():
  REF: https://stackoverflow.com/questions/63464333/how-to-calcuate-the-overlap-date-in-pyspark

  Target: find total years of experience based on several date ranges with any overlaps and/or gaps
  Method: 
    (1) convert `Experience_datesEmployeed` into two columns: start_date and end_date
    (2) find the prev_max_date = max('end_date').over(w1) where w1 is from unboundedPreceding up to the previous row
    (3) find the effective month_diff = `greatest(end_date,prev_max_date)` - `greatest(start_date,prev_max_date)`
    (4) groupby and find the sum of month_diff

  Code:

    from pyspark.sql.functions import max as fmax, expr
    from pyspark.sql import Window

    df = spark.createDataFrame([
       ('David', 'Feb 1999 - Sep 2001', 'Foothill', '2 yrs 8 mos', 'Marketing Assoicate'),
       ('David', '1994 - 1997', ' abc', '3 yrs', 'Senior Auditor'),
       ('David', 'Jun 2020 - Present', 'Fellows INC', '3 mos', 'Director Board'),
       ('David', '2017 - Jun 2019', 'Fellows INC', '2 yrs', 'Fellow - Class 22'),
       ('David', 'Sep 2001 - Present', 'The John D.', '19 yrs', 'Manager')
     ], ['fullName', 'Experience_datesEmployeed', 'expcompany', 'expduraation', 'position'])

  Step-1: find start_date and end_date of each Experience_datesEmployeed:

    df1 = (df.withColumn('dates', expr("split(Experience_datesEmployeed, ' *- *')")) 
        .withColumn('start_date', expr("coalesce(to_date(dates[0], 'MMM yyyy'), to_date(dates[0], 'yyyy'))")) 
        .withColumn('end_date', expr("""
            IF(dates[1] = 'Present', 
               current_date(), 
               last_day(coalesce(to_date(dates[1], 'MMM yyyy'), to_date(dates[1], 'yyyy') + interval 11 months)))
         """))
        .select('fullName', 'Experience_datesEmployeed','dates','start_date','end_date')
         ) 
         
    df1.show() 
    +--------+-------------------------+--------------------+----------+----------+
    |fullName|Experience_datesEmployeed|               dates|start_date|  end_date|
    +--------+-------------------------+--------------------+----------+----------+
    |   David|      Feb 1999 - Sep 2001|[Feb 1999, Sep 2001]|1999-02-01|2001-09-30|
    |   David|              1994 - 1997|        [1994, 1997]|1994-01-01|1997-12-31|
    |   David|       Jun 2020 - Present| [Jun 2020, Present]|2020-06-01|2020-08-18|
    |   David|          2017 - Jun 2019|    [2017, Jun 2019]|2017-01-01|2019-06-30|
    |   David|       Sep 2001 - Present| [Sep 2001, Present]|2001-09-01|2020-08-18|
    +--------+-------------------------+--------------------+----------+----------+

  Step-2: find effctive_month_diff based on start_date, end_date and prev_max_date:

    w1 = Window.partitionBy('fullName').orderBy('start_date').rowsBetween(Window.unboundedPreceding,-1)

    df2 = (df1.withColumn('prev_max_date', fmax('end_date').over(w1)) 
        .withColumn("effctive_month_diff", expr("""
           round(months_between(greatest(end_date,prev_max_date),greatest(start_date,prev_max_date)),0)
         """)))
    +--------+-------------------------+--------------------+----------+----------+-------------+--------------------+
    |fullName|Experience_datesEmployeed|               dates|start_date|  end_date|prev_max_date|effective_month_diff|
    +--------+-------------------------+--------------------+----------+----------+-------------+--------------------+
    |   David|              1994 - 1997|        [1994, 1997]|1994-01-01|1997-12-31|         null|                48.0|
    |   David|      Feb 1999 - Sep 2001|[Feb 1999, Sep 2001]|1999-02-01|2001-09-30|   1997-12-31|                32.0|
    |   David|       Sep 2001 - Present| [Sep 2001, Present]|2001-09-01|2020-08-18|   2001-09-30|               227.0|
    |   David|          2017 - Jun 2019|    [2017, Jun 2019]|2017-01-01|2019-06-30|   2020-08-18|                 0.0|
    |   David|       Jun 2020 - Present| [Jun 2020, Present]|2020-06-01|2020-08-18|   2020-08-18|                 0.0|
    +--------+-------------------------+--------------------+----------+----------+-------------+--------------------+

  Step-3: find Total_experience:

    df2.groupby("fullName").agg(expr("round(sum(effctive_month_diff)/12,1) as Total_experience")).show()
    +--------+----------------+                                                     
    |fullName|Total_experience|
    +--------+----------------+
    |   David|            25.6|
    +--------+----------------+

        
    
