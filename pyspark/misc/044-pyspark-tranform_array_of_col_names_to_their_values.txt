Question

https://stackoverflow.com/questions/57086759/how-to-get-a-list-column-with-values-of-multiple-columns-given-in-another-column

For Spark 2.4.0+, use the builtin SQL function transform():

Data Setup
----------

    myValues = [(1,2,0,3,4,['col1','col2']),(1,2,0,3,4,['col2','col3']),
                (1,2,0,3,4,['col1','col3']),(1,2,0,3,4,['col3','col4']),
                (1,2,0,3,4,['col2','col5']),(1,2,0,3,4,['col4','col5'])]

    df = spark.createDataFrame(myValues,['col1','col2','col3','col4','col5','colList'])

    >>> df.show()
    +----+----+----+----+----+------------+
    |col1|col2|col3|col4|col5|     colList|
    +----+----+----+----+----+------------+
    |   1|   2|   0|   3|   4|[col1, col2]|
    |   1|   2|   0|   3|   4|[col2, col3]|
    |   1|   2|   0|   3|   4|[col1, col3]|
    |   1|   2|   0|   3|   4|[col3, col4]|
    |   1|   2|   0|   3|   4|[col2, col5]|
    |   1|   2|   0|   3|   4|[col4, col5]|
    +----+----+----+----+----+------------+

    # columns that involved in colList    
    cols = df.columns[1:5]
    
Method-1: use Spark SQL CASE/WHEN

Set up the SQL statement
------------------------
SQL statement to map column names in colList into the corresponding column fields

**Note:** use CONCAT(x, '') to convert x into a String and solve the issue with data type mismatch ERROR

    stmt = '''
    
        transform(colList, x -> 
            CASE CONCAT(x, '')
                WHEN 'col1' THEN `col1`
                WHEN 'col2' THEN `col2`
                WHEN 'col3' THEN `col3`
                WHEN 'col4' THEN `col4`
                WHEN 'col5' THEN `col5`
                ELSE NULL
            END
        )
    
    '''

Run the SQL with F.expr():
--------------------------

    df_new = df.withColumn('new_list', F.expr(stmt))

    >>> df_new.show()
    +----+----+----+----+----+------------+--------+
    |col1|col2|col3|col4|col5|     colList|new_list|
    +----+----+----+----+----+------------+--------+
    |   1|   2|   0|   3|   4|[col1, col2]|  [1, 2]|
    |   1|   2|   0|   3|   4|[col2, col3]|  [2, 0]|
    |   1|   2|   0|   3|   4|[col1, col3]|  [1, 0]|
    |   1|   2|   0|   3|   4|[col3, col4]|  [0, 3]|
    |   1|   2|   0|   3|   4|[col2, col5]|  [2, 4]|
    |   1|   2|   0|   3|   4|[col4, col5]|  [3, 4]|
    +----+----+----+----+----+------------+--------+


Dynamically generate SQL statement:
-----------------------------------
For more columns, use the following code to dynamically create the stmt:

    when_cases = '\n'.join([ "WHEN '{0}' THEN `{0}`".format(c) for c in cols ])

    stmt = '''transform(colList, x -> CASE CONCAT(x, '') {0} ELSE NULL END)'''.format(when_cases)

    >>> print(stmt)
    transform(colList, x -> CASE CONCAT(x, '') WHEN 'col1' THEN `col1`
    WHEN 'col2' THEN `col2`
    WHEN 'col3' THEN `col3`
    WHEN 'col4' THEN `col4`
    WHEN 'col5' THEN `col5` ELSE NULL END)


Some notes:
-----------
  
  + ERROR due to data type mismatch: THEN and ELSE expressions should all be same type or coercible to a common type: 
    (namedlambdavariable() = 'col1')   
    + String(x) == 'col1'     <-- not working
    + CONCAT(x, '') == 'col1'    <-- works

  + col1 as String or Fieldname, use the WHEN/CASE statement to do the mapping

###############################
Method-2: use mapping and transform (Only for spark 2.4+)
    
    """
       map_cols  : create mapping from column-name -> column-index
       arr       : combine all related columns into an array
       list_vals : use transform() to convert array of column names to their corresponding values
    """
    df.withColumn('map_cols', F.map_from_arrays(
             F.array(*[F.lit(c) for c in cols])
           , F.sequence(F.lit(0), F.lit(len(cols)-1))
       )) \
      .withColumn('arr', F.array(cols)) \
      .withColumn('list_vals', F.expr('transform(colList, c -> arr[map_cols[c]])')) \
      .drop('map_cols', 'arr') \
      .show()
    +----+----+----+----+----+------------+---------+
    |col1|col2|col3|col4|col5|     colList|list_vals|
    +----+----+----+----+----+------------+---------+
    |   1|   2|   0|   3|   4|[col1, col2]|   [1, 2]|
    |   1|   2|   0|   3|   4|[col2, col3]|   [2, 0]|
    |   1|   2|   0|   3|   4|[col1, col3]|   [1, 0]|
    |   1|   2|   0|   3|   4|[col3, col4]|   [0, 3]|
    |   1|   2|   0|   3|   4|[col2, col5]|   [2, 4]|
    |   1|   2|   0|   3|   4|[col4, col5]|   [3, 4]|
    +----+----+----+----+----+------------+---------+

"""
An Extra question from SO (using the same approach):
https://stackoverflow.com/questions/57762615/how-to-concat-values-of-columns-in-pyspark
"""
    from pyspark.sql.functions import split, expr, concat_ws

    map_cols = {'a':'newA', 'b':'newB', 'c':'newC', 'd':'newD'}
   
    df = spark.createDataFrame([
          ('a_b', '1', '2', '7', '8') 
        , ('a_b_c', '2', '3', '4', '4') 
        , ('a_b_c_d', '3', '2', '4', '4') 
        , ('c_d', '89', '5', '3', '5') 
        , ('b_c_d', '7', '5', '6', '5') 
      ], ['names', 'newA', 'newB', 'newC', 'newD'])

    when_cases = '\n'.join([ "WHEN '{0}' THEN `{1}`".format(k,v) for k,v in map_cols.items() ])
    stmt = '''transform(names_arr, x -> CASE CONCAT(x, '') {0} ELSE NULL END)'''.format(when_cases)

    print(stmt)
    #transform(names_arr, x -> CASE CONCAT(x, '') WHEN 'a' THEN `newA`
    #WHEN 'c' THEN `newC`
    #WHEN 'b' THEN `newB`
    #WHEN 'd' THEN `newD` ELSE NULL END)

    df.withColumn('names_arr', split('names', '_')).withColumn('result', expr(stmt)).show()
    +-------+----+----+----+----+------------+------------+
    |  names|newA|newB|newC|newD|   names_arr|      result|
    +-------+----+----+----+----+------------+------------+
    |    a_b|   1|   2|   7|   8|      [a, b]|      [1, 2]|
    |  a_b_c|   2|   3|   4|   4|   [a, b, c]|   [2, 3, 4]|
    |a_b_c_d|   3|   2|   4|   4|[a, b, c, d]|[3, 2, 4, 4]|
    |    c_d|  89|   5|   3|   5|      [c, d]|      [3, 5]|
    |  b_c_d|   7|   5|   6|   5|   [b, c, d]|   [5, 6, 5]|
    +-------+----+----+----+----+------------+------------+

    # using concat_ws(';') to concatenate array items
    df_new = df \
        .withColumn('names_arr', split('names', '_')) \
        .withColumn('result', concat_ws(';', expr(stmt))) \
        .drop('names_arr')

    df_new.show()
    +-------+----+----+----+----+-------+
    |  names|newA|newB|newC|newD| result|
    +-------+----+----+----+----+-------+
    |    a_b|   1|   2|   7|   8|    1;2|
    |  a_b_c|   2|   3|   4|   4|  2;3;4|
    |a_b_c_d|   3|   2|   4|   4|3;2;4;4|
    |    c_d|  89|   5|   3|   5|    3;5|
    |  b_c_d|   7|   5|   6|   5|  5;6;5|
    +-------+----+----+----+----+-------+
