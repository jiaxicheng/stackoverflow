https://stackoverflow.com/questions/58048745/how-to-detect-if-decimal-columns-should-be-converted-into-integer-or-double

Find all column names with datetype == DecimalType() 

Using the schema of the dataframe: `df.schema.jsonValue()['fields']`

    from pyspark.sql.functions import col

    df = spark.createDataFrame([ (1, 12.3, 1.5, 'test', 13.23) ], ['i1', 'd2', 'l3', 's4', 'd5'])

    df = df.withColumn('d2', col('d2').astype('decimal(10,1)')) \
           .withColumn('d5', col('d5').astype('decimal(10,2)'))

    #DataFrame[i1: bigint, d2: decimal(10,1), l3: double, s4: string, d5: decimal(10,2)]

    decimal_cols = [ f['name'] for f in df.schema.jsonValue()['fields'] if f['type'].startswith('decimal') ]
    
    print(decimal_cols)
    ['d2', 'd5']

However the above will not work if decimal fields are saved in nested data structures. in such case,
you might probably modify the schema directly and then reload the data into a new dataframe. 

    from pyspark.sql.types import StructType
    import re
    import json

    df1 = df.withColumn('a3', F.array('d2','d5'))    
    # DataFrame[i1: bigint, d2: decimal(10,1), f3: double, s4: string, d5: decimal(10,2), a3: array<decimal(11,2)>]

    # export schema into JSON string
    old_schema_json = df1.schema.json()
    
    # use regex to modify all decimal(d,d) to double
    new_schema_json = re.sub('"decimal\(\d+,\d+\)"', '"double"', old_schema_json)

    # convert the new json string into StructType
    new_schema = StructType.fromJson(json.loads(new_schema_json))
    print(new_schema.simpleString())
    # 'struct<i1:bigint,d2:double,f3:double,s4:string,d5:double,a3:array<double>>'

    df2 = spark.createDataFrame(df1.rdd, new_schema)
    # DataFrame[i1: bigint, d2: double, f3: double, s4: string, d5: double, a3: array<double>]

