https://stackoverflow.com/questions/59931770/sum-of-array-elements-depending-on-value-condition-pyspark/59938994#59938994

Use pivot, specify list of values to pivot, this is good for performance:

    from pyspark.sql.functions import sum as fsum, expr

    df = spark.createDataFrame([
        (1,[0.2, 2.1, 3., 4., 3., 0.5]),
        (2,[7., 0.3, 0.3, 8., 2.,]), 
        (3,None), 
        (4,[])
      ],['id','column'])
    df.show()
    +---+--------------------+
    | id|              column|
    +---+--------------------+
    |  1|[0.2, 2.1, 3.0, 4...|
    |  2|[7.0, 0.3, 0.3, 8...|
    |  3|                null|
    |  4|                  []|
    +---+--------------------+

    df.selectExpr('id', 'explode_outer(column) as item') \
      .withColumn('g', expr('if(item < 2, "column<2", if(item > 2, "column>2", "column=2"))')) \
      .groupby('id') \
      .pivot('g', ["column<2", "column>2", "column=2"]) \
      .agg(fsum('item')) \
      .show()
    +---+--------+--------+--------+                                                
    | id|column<2|column>2|column=2|
    +---+--------+--------+--------+
    |  1|     0.7|    12.1|    null|
    |  3|    null|    null|    null|
    |  2|     0.6|    15.0|     2.0|
    |  4|    null|    null|    null|
    +---+--------+--------+--------+
