use MapType:
---

Example-1: create dict using `rdd.collectMap()` and use map
  REF: https://stackoverflow.com/questions/61823544/pyspark-mapping-multiple-columns/61829008#61829008
  Method: 
   (1) create a Map from the dataframe `reference_df` with:

        map_key = concat_ws('\0', PrimaryLookupAttributeName, PrimaryLookupAttributeValue)
        map_value = OutputItemNameByValue

      and then apply it to another dataframe `df1` with the key: 

        concat_ws('\0', <col_name>, <col_value>) 
    
      where `col` is from a List `primaryLookupAttributeName_List`

   (2) create a dictionary using: 
   
       df.select(key_col, value_col).rdd.collectAsMap()

  Code:
    
    from pyspark.sql.functions import collect_set, array, concat_ws, lit, col, create_map
   
    reference_df = spark.read.csv("/home/xicheng/test/join-8.txt", header=True)
    df1 = spark.read.csv("/home/xicheng/test/join-8-1.txt", header=True)

    primaryLookupAttributeName_List = ['LeaseType', 'LeaseRecoveryType', 'LeaseStatus']
    
    d = reference_df.select(
        concat_ws('\0','PrimaryLookupAttributeName','PrimaryLookupAttributeValue'), 'OutputItemNameByValue'
    ).rdd.collectAsMap().items()
    #[['LeaseStatus\x00Abandoned', 'Active'],
    # ['LeaseStatus\x00DEFAULT', 'Pending'],
    # ['LeaseRecoveryType\x00Gross-modified', 'Modified Gross'],
    # ['LeaseStatus\x00Archive', 'Expired'],
    # ['LeaseStatus\x00Terminated', 'Terminated'],
    # ['LeaseRecoveryType\x00Gross w/base year', 'Modified Gross'],
    # ['LeaseRecoveryType\x00Gross', 'Gross']]
    
    # create mapping based on the above list of lists
    mappings = create_map([lit(j) for i in d for j in i ])

    # default_mappings:
    mappings_default = create_map([ lit(j.split('\0')[0]) for i in d if i[0].upper().endswith('\x00DEFAULT') for j in i ])
    #Column<b'map(LeaseStatus, Pending)'>

    # a set of available PrimaryLookupAttributeName
    available_list = set([ i[0].split('\0')[0] for i in d ])
    # {'LeaseRecoveryType', 'LeaseStatus'}

    df1.select("*", *[ 
      coalesce(
        mappings[concat_ws('\0', lit(c), col(c))],
        mappings_default[c],
        lit("Not Specified at Source" if c in available_list else "Lookup not found")
      ).alias("Matched[{}]OutputItemNameByValue".format(c)) for c in primaryLookupAttributeName_List ]
    ).show()
    +----------------+...+---------------------------------------+-----------------------------------------------+-----------------------------------------+
    |SourceSystemName|...|Matched[LeaseType]OutputItemNameByValue|Matched[LeaseRecoveryType]OutputItemNameByValue|Matched[LeaseStatus]OutputItemNameByValue|
    +----------------+...+---------------------------------------+-----------------------------------------------+-----------------------------------------+
    |          ABC123|...|                       Lookup not found|                                          Gross|                               Terminated|
    |          ABC123|...|                       Lookup not found|                                 Modified Gross|                                  Expired|
    |          ABC123|...|                       Lookup not found|                                 Modified Gross|                                  Pending|
    +----------------+...+---------------------------------------+-----------------------------------------------+-----------------------------------------+


 
Example-2: create map using SQL syntax and map_from_entries + transform
  REF: https://stackoverflow.com/questions/63774092
  Task: create new columns based on col2 and its values mapped from 5 sets
    
    from pyspark.sql.functions import expr, split
    
    df = spark.createDataFrame([
      (1, 'b1, a1, c1'), (2, 'a2, b2'), (3, 'e3, d3, a3, c3, b3')
    ], ['col1', 'col2'])
    
    set1 = ['a1', 'a2', 'a3', 'a4', 'a5'] 
    set2 = ['b1', 'b2', 'b3', 'b4', 'b5'] 
    set3 = ['c1', 'c2', 'c3', 'c4', 'c5'] 
    set4 = ['d1', 'd2', 'd3', 'd4', 'd5'] 
    set5 = ['e1', 'e2', 'e3', 'e4', 'e5']
    
    sets = [set1, set2, set3, set4, set5]
    
    # be caution if any elements in sets contain single quotes.
    map1 = f"""map({','.join(f"'{e}', {i}" for i,s in enumerate(sets) for e in s)})"""
    # "map('a1', 0,'a2', 0,'a3', 0,'a4', 0,'a5', 0,'b1', 1,'b2', 1,'b3', 1,'b4', 1,'b5', 1,'c1', 2
    # ,'c2', 2,'c3', 2,'c4', 2,'c5', 2,'d1', 3,'d2', 3,'d3', 3,'d4', 3,'d5', 3,'e1', 4,'e2', 4
    # ,'e3', 4,'e4', 4,'e5', 4)"
    
    map2 = expr(f"map_from_entries(transform(data, x -> ({map1}[x],x)))")
    
    df_new = (df.withColumn('data', split('col2', r'\s*,\s*')) 
        .select('col1', *[ map2[i].alias(f"col{i+3}") for i in range(5) ]) 
    ) 
    +----+----+----+----+----+----+
    |col1|col3|col4|col5|col6|col7|
    +----+----+----+----+----+----+
    |   1|  a1|  b1|  c1|null|null|
    |   2|  a2|  b2|null|null|null|
    |   3|  a3|  b3|  c3|  d3|  e3|
    +----+----+----+----+----+----+
    


Example-3: create a map with array of string as values:
  REF: https://stackoverflow.com/questions/63788176
  Task: set a new column with true/false if brand + type combo is shown in a list.
  Method: use create_map and list comprehension to create a map<string,array<string>>
          then do arrays_overlap

    df = spark.createDataFrame([
      ["Apple",['iPhone EE','iPhone 11', 'iPhone 11 Pro']],   
      ["Acer",['Iconia Talk S','liquid Z6 Plus']],   
      ["Casio",['Casio G\'zOne Brigade']],
      ["Alcatel",[]]
    ]).toDF("brand","type")


    from pyspark.sql.functions import arrays_overlap, array, lit, col, create_map
    
    dict1 = {'Casio': ["Casio G'zOne Ravine"],
             'Alcatel': ['3L'],
             'Acer': ['Acer Predator 8', 'liquid Z6 Plus'],
             'Apple': ['iPhone EE', 'iPhone 11 Pro', 'iPhone XS']}

    # create a map<string,array<string>> with brand as key and array of types as value
    map1 = create_map([ t for k,v in dict1.items() for t in [lit(k), array(*map(lit,v)] ])
    #Column<b"map(Casio, array(Casio G'zOne Ravine), Alcatel, array(3L), Acer, array(Acer Predator 8, liquid Z6 Plus), Apple, array(iPhone EE, iPhone 11 Pro, iPhone XS))">

    df.withColumn('Match', arrays_overlap('type', map1[col('brand')])).show(5,0)
    +-------+-------------------------------------+-----+
    |brand  |type                                 |Match|
    +-------+-------------------------------------+-----+
    |Apple  |[iPhone EE, iPhone 11, iPhone 11 Pro]|true |
    |Acer   |[Iconia Talk S, liquid Z6 Plus]      |true |
    |Casio  |[Casio G'zOne Brigade]               |false|
    |Alcatel|[]                                   |false|
    +-------+-------------------------------------+-----+

  Note: if None (or NULL) is allowed in the list of types, arrays_overlap might return NULL, in such case, add 
        `coalesce(arrays_overlap(a1,a2), lit(False))`, or use `size(array_intersect(a1,a2)) > 0` to replace 
        `arrays_overlap(a1,a2)`



Example-4: convert K, M, G into corresponding numbers
  REF: https://stackoverflow.com/questions/64003713

    from pyspark.sql.functions import translate, coalesce, lit, substring, expr

    df = spark.createDataFrame([(e,) for e in ["3M", "2K", "4500", ".12M"]],["col1"])

    scale_map = expr("map('K',1000, 'M',1000000, 'G', 1000000000)")

    df_new = df.withColumn('col2', translate('col1', 'KMG', '') * coalesce(scale_map[substring('col1',-1,1)],lit(1)))
    df_new.show()
    +----+---------+
    |col1|     col2|
    +----+---------+
    |  3M|3000000.0|
    |  2K|   2000.0|
    |4500|   4500.0|
    |.12M| 120000.0|
    +----+---------+

Example-4.2: same as example-4
  REF: https://stackoverflow.com/questions/64039632

    from pyspark.sql.functions import translate, coalesce, lit, substring, expr

    df = spark.createDataFrame([
          ('132K', '224.4M', '11160K', '0', '224.4M', '11160K', '0K') 
        , ('134M', '224.9K', '12260K', '0', '224.4M', '11160K', '0K') 
        , ('132K', '225.5M', '11160K', '0', '224.4M', '11160K', '0K') 
    ], ['MINFLT', 'MAJFLT', 'VSTEXT', 'VSIZE', 'RSIZE', 'VGROW', 'RGROW']) 
    +------+------+------+-----+------+------+-----+
    |MINFLT|MAJFLT|VSTEXT|VSIZE| RSIZE| VGROW|RGROW|
    +------+------+------+-----+------+------+-----+
    |  132K|224.4M|11160K|    0|224.4M|11160K|   0K|
    |  134M|224.9K|12260K|    0|224.4M|11160K|   0K|
    |  132K|225.5M|11160K|    0|224.4M|11160K|   0K|
    +------+------+------+-----+------+------+-----+

    scale_map = expr("map('K',1000, 'M',1000000, 'G', 1000000000)")

    cols_included = {'MAJFLT', 'RSIZE'}

    df_new = df.select([ 
      (translate(c, 'KMG', '')*coalesce(scale_map[substring(c,-1,1)],lit(1))).astype('bigint').alias(c) 
        if c in cols_included else c for c in df.columns ])

    df_new.show()
    +------+---------+------+-----+---------+------+-----+
    |MINFLT|   MAJFLT|VSTEXT|VSIZE|    RSIZE| VGROW|RGROW|
    +------+---------+------+-----+---------+------+-----+
    |  132K|224400000|11160K|    0|224400000|11160K|   0K|
    |  134M|   224900|12260K|    0|224400000|11160K|   0K|
    |  132K|225500000|11160K|    0|224400000|11160K|   0K|
    +------+---------+------+-----+---------+------+-----+



Example-5: use map + array, notice that in DSL mode, both array indices and map keys can be setup with a Column type. 
  REF: https://stackoverflow.com/questions/63966039

    from pyspark.sql.functions import create_map, array, col, lit

    df = spark.createDataFrame([
      (123,"A","3312019"),(123,"B","3312019"),(123,"A","12312019"),(123, "B", "12312019"),
      (123,"B","5302020"), (123,"D","5302020")
    ],['ID', 'Category', 'Date']) 

    df2_1 = spark.createDataFrame([
      (123,"3312019",40,60,None,None),(123,"12312019",20,None,None,None),(123,"5302020",30,10,None,None)
    ], schema='ID int, Date string, A int, B int, C int, D int')

    cols = ["A", "B", "C", "D"]
    map1 = create_map([lit(e) for i,x in enumerate(cols) for e in [x, i]])
    +---+--------+----------+

    df_new = df.join(df2_1.select("ID", "Date", array(cols).alias("arr")), ["ID","Date"], "left") \
        .select(*df.columns, col('arr')[map1[col("Category")]].alias("Points1"))
        
    df_new.show()
    +---+--------+--------+-------+                                                 
    | ID|Category|    Date|Points1|
    +---+--------+--------+-------+
    |123|       A|12312019|     20|
    |123|       B|12312019|   null|
    |123|       B| 5302020|     10|
    |123|       D| 5302020|   null|
    |123|       A| 3312019|     40|
    |123|       B| 3312019|     60|
    +---+--------+--------+-------+


